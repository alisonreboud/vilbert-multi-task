{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare pkl cache file for the deep coption (dc) textual input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the deep captions .csv file and initialize the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_coptions_path = \"/aloui/MediaEval/alto_titles_danny.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>caption</th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>man playing guitar singing snow-and-string-lig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bird sitting cage cage pan-to-cat-in-animal-sh...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>man sitting bed smiling blonde-woman-is-massag...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>man playing piano room roulette-table-spinning...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>group people walking along beach snow-shoe-hik...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            caption  video\n",
       "0           0  man playing guitar singing snow-and-string-lig...      1\n",
       "1           1  bird sitting cage cage pan-to-cat-in-animal-sh...      2\n",
       "2           2  man sitting bed smiling blonde-woman-is-massag...      3\n",
       "3           3  man playing piano room roulette-table-spinning...      4\n",
       "4           4  group people walking along beach snow-shoe-hik...      5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_coptions_df = pd.read_csv(deep_coptions_path)\n",
    "deep_coptions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_entries = []\n",
    "for r in deep_coptions_df.itertuples():\n",
    "    sample = {}\n",
    "    vid_id = int(r.video)\n",
    "    caption = r.caption.rstrip().replace('-', ' ')\n",
    "    sample['video_id'] = vid_id\n",
    "    sample['caption'] = caption\n",
    "    dc_entries.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>short-term_memorability</th>\n",
       "      <th>nb_short-term_annotations</th>\n",
       "      <th>long-term_memorability</th>\n",
       "      <th>nb_long-term_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video10.webm</td>\n",
       "      <td>0.950</td>\n",
       "      <td>34</td>\n",
       "      <td>0.900</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video100.webm</td>\n",
       "      <td>0.951</td>\n",
       "      <td>33</td>\n",
       "      <td>0.889</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video10000.webm</td>\n",
       "      <td>0.832</td>\n",
       "      <td>33</td>\n",
       "      <td>1.000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video10001.webm</td>\n",
       "      <td>0.865</td>\n",
       "      <td>33</td>\n",
       "      <td>0.727</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video10002.webm</td>\n",
       "      <td>0.899</td>\n",
       "      <td>59</td>\n",
       "      <td>0.792</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             video  short-term_memorability  nb_short-term_annotations  \\\n",
       "0     video10.webm                    0.950                         34   \n",
       "1    video100.webm                    0.951                         33   \n",
       "2  video10000.webm                    0.832                         33   \n",
       "3  video10001.webm                    0.865                         33   \n",
       "4  video10002.webm                    0.899                         59   \n",
       "\n",
       "   long-term_memorability  nb_long-term_annotations  \n",
       "0                   0.900                        10  \n",
       "1                   0.889                         9  \n",
       "2                   1.000                        13  \n",
       "3                   0.727                        11  \n",
       "4                   0.792                        24  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/aloui/MediaEval/dev-set/ground-truth/ground-truth_dev-set.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dict = {}\n",
    "for r in train_df.itertuples():\n",
    "    vid_id = re.findall(r'\\d+', r.video)[0]\n",
    "    vid_id = int(vid_id)\n",
    "    score_dict[vid_id] = [r._2, r._4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score_list = []\n",
    "train_score_list = []\n",
    "\n",
    "for sample in dc_entries:\n",
    "    try:\n",
    "        sample['scores'] = score_dict[sample['video_id']]\n",
    "        train_score_list.append(sample)\n",
    "    except:\n",
    "        test_score_list.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_score_list), len(test_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'video_id': 3,\n",
       "  'caption': 'man sitting bed smiling blonde woman is massaged tilt down person bed',\n",
       "  'scores': [0.924, 0.846]},\n",
       " {'video_id': 1,\n",
       "  'caption': 'man playing guitar singing snow and string lights person playing video game'})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score_list[0], test_score_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_eq(real, expected):\n",
    "    assert real == expected, \"%s (true) vs %s (expected)\" % (real, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same tokenize function from BERT\n",
    "\n",
    "def tokenize(entries, tokenizer, max_length=16, padding_index=0):\n",
    "    \"\"\"Tokenizes the questions.\n",
    "\n",
    "    This will add q_token in each entry of the dataset.\n",
    "    -1 represent nil, and should be treated as padding_index in embedding\n",
    "    \"\"\"\n",
    "    for entry in entries:\n",
    "        tokens = tokenizer.encode(entry[\"caption\"])\n",
    "        tokens = tokens[: max_length - 2]\n",
    "        tokens = tokenizer.add_special_tokens_single_sentence(tokens)\n",
    "\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        input_mask = [1] * len(tokens)\n",
    "\n",
    "        if len(tokens) < max_length:\n",
    "            # Note here we pad in front of the sentence\n",
    "            padding = [padding_index] * (max_length - len(tokens))\n",
    "            tokens = tokens + padding\n",
    "            input_mask += padding\n",
    "            segment_ids += padding\n",
    "\n",
    "        assert_eq(len(tokens), max_length)\n",
    "        entry[\"c_token\"] = tokens\n",
    "        entry[\"c_input_mask\"] = input_mask\n",
    "        entry[\"c_segment_ids\"] = segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 23\n",
    "\n",
    "tokenize(train_score_list, tokenizer, max_length=max_length)\n",
    "tokenize(test_score_list, tokenizer, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['video_id', 'caption', 'scores', 'c_token', 'c_input_mask', 'c_segment_ids'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score_list[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same tensorize function from BERT\n",
    "\n",
    "def tensorize(entries, split='trainval'):\n",
    "\n",
    "    for entry in entries:\n",
    "        caption = torch.from_numpy(np.array(entry[\"c_token\"]))\n",
    "        entry[\"c_token\"] = caption\n",
    "\n",
    "        c_input_mask = torch.from_numpy(np.array(entry[\"c_input_mask\"]))\n",
    "        entry[\"c_input_mask\"] = c_input_mask\n",
    "\n",
    "        c_segment_ids = torch.from_numpy(np.array(entry[\"c_segment_ids\"]))\n",
    "        entry[\"c_segment_ids\"] = c_segment_ids\n",
    "\n",
    "        if \"test\" not in split:\n",
    "            # answer = entry[\"answer\"]\n",
    "            # labels = np.array(answer[\"labels\"])\n",
    "            scores = np.array(entry[\"scores\"], dtype=np.float32)\n",
    "\n",
    "            scores = torch.from_numpy(scores)\n",
    "            entry[\"scores\"] = scores\n",
    "            # entry[\"answer\"][\"scores\"] = scores\n",
    "\n",
    "            '''if len(labels):\n",
    "                labels = torch.from_numpy(labels)\n",
    "                scores = torch.from_numpy(scores)\n",
    "                entry[\"answer\"][\"labels\"] = labels\n",
    "                entry[\"answer\"][\"scores\"] = scores\n",
    "            else:\n",
    "                entry[\"answer\"][\"labels\"] = None\n",
    "                entry[\"answer\"][\"scores\"] = None'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 'trainval'\n",
    "val_split = 'minval'\n",
    "test_split = 'test'\n",
    "dataroot = 'datasets/ME'\n",
    "train_cache_path = os.path.join(dataroot, 'cache', 'ME' + '_' + train_split + '_' + str(max_length) + 'dc_' + 'cleaned' + '.pkl')\n",
    "val_cache_path = os.path.join(dataroot, 'cache', 'ME' + '_' + val_split + '_' + str(max_length) + 'dc_' + 'cleaned' + '.pkl')\n",
    "test_cache_path = os.path.join(dataroot, 'cache', 'ME' + '_' + test_split + '_' + str(max_length) + 'dc_' + 'cleaned' + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorize(train_score_list, split='trainval')\n",
    "tensorize(test_score_list, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_id': 1,\n",
       " 'caption': 'man playing guitar singing snow and string lights person playing video game',\n",
       " 'c_token': tensor([ 101, 2158, 2652, 2858, 4823, 4586, 1998, 5164, 4597, 2711, 2652, 2678,\n",
       "         2208,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
       " 'c_input_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'c_segment_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this done only once to save entries to the disk\n",
    "cPickle.dump(train_score_list, open(train_cache_path, 'wb'))\n",
    "# cPickle.dump(val_entries, open(val_cache_path, 'wb'))\n",
    "cPickle.dump(test_score_list, open(test_cache_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract 30 frames from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_dir = '/aloui/MediaEval/dev-set/sources/'\n",
    "test_video_dir = '/aloui/MediaEval/test-set/sources/'\n",
    "\n",
    "train_image_dir = 'datasets/ME/images/dc/train/'\n",
    "test_image_dir = 'datasets/ME/images/dc/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [7:50:25<00:00,  3.94s/it]  \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_im_dict = dict()\n",
    "\n",
    "for k, filename in enumerate(tqdm(os.listdir(train_video_dir))):\n",
    "    if filename.endswith(\".webm\"):\n",
    "        vid_id = re.findall(r'\\d+', filename)[0]\n",
    "        vid_id = int(vid_id)\n",
    "        video_path = os.path.join(train_video_dir, filename)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frameIds = cap.get(cv2.CAP_PROP_FRAME_COUNT) * np.random.uniform(size=30)\n",
    "        for i, fid in enumerate(frameIds):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n",
    "            ret, frame = cap.read()\n",
    "            cv2.imwrite(train_image_dir + str(vid_id) + '_' + str(i) + '.jpg', frame)\n",
    "        # plt.imshow(cv2.cvtColor(averageFrame, cv2.COLOR_BGR2RGB))\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [1:56:47<00:00,  4.01s/it]  \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "test_im_dict = dict()\n",
    "\n",
    "for k, filename in enumerate(tqdm(os.listdir(test_video_dir))):\n",
    "    if filename.endswith(\".webm\"):\n",
    "        vid_id = re.findall(r'\\d+', filename)[0]\n",
    "        vid_id = int(vid_id)\n",
    "        video_path = os.path.join(test_video_dir, filename)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frameIds = cap.get(cv2.CAP_PROP_FRAME_COUNT) * np.random.uniform(size=30)\n",
    "        for i, fid in enumerate(frameIds):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n",
    "            ret, frame = cap.read()\n",
    "            cv2.imwrite(test_image_dir + str(vid_id) + '_' + str(i) + '.jpg', frame)\n",
    "        # plt.imshow(cv2.cvtColor(averageFrame, cv2.COLOR_BGR2RGB))\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
