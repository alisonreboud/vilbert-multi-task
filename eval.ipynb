{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/tfutils/optimizer.py:18: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from io import open\n",
    "import numpy as np\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from bisect import bisect\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "import sys\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from vilbert.task_utils import (\n",
    "    LoadDatasetEval,\n",
    "    LoadLosses,\n",
    "    ForwardModelsTrain,\n",
    "    ForwardModelsVal,\n",
    "    EvaluatingModel,\n",
    ")\n",
    "\n",
    "import vilbert.utils as utils\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2020 15:50:52 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "07/28/2020 15:50:52 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/aloui/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "07/28/2020 15:50:52 - INFO - vilbert.task_utils -   Loading VQA Dataset with batch size 30\n",
      "07/28/2020 15:50:52 - INFO - vilbert.datasets.vqa_dataset -   Loading from datasets/VQA/cache/VQA_minval_23_cleaned.pkl\n",
      "07/28/2020 15:50:53 - INFO - vilbert.utils -   logging file at: pytorch_model_19.bin-testing\n",
      "07/28/2020 15:50:53 - INFO - vilbert.utils -   loading weights file save/VQA_bert_base_6layer_6conect-finetune_from_multi_task_model/pytorch_model_19.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num Iters:  {'TASK1': 100}\n",
      "  Batch size:  {'TASK1': 30}\n",
      "99/100\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2020 15:51:12 - INFO - vilbert.utils -   Eval task TASK1 on iteration 0 \n",
      "07/28/2020 15:51:12 - INFO - vilbert.utils -   Validation [VQA]: loss 0.000 score 0.000 \n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--bert_model\",\n",
    "    default=\"bert-base-uncased\",\n",
    "    type=str,\n",
    "    help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "    \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--from_pretrained\",\n",
    "    default=\"bert-base-uncased\",\n",
    "    type=str,\n",
    "    help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
    "    \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    default=\"results\",\n",
    "    type=str,\n",
    "    help=\"The output directory where the model checkpoints will be written.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config_file\",\n",
    "    default=\"config/bert_config.json\",\n",
    "    type=str,\n",
    "    help=\"The config file which specified the model details.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no_cuda\", action=\"store_true\", help=\"Whether not to use CUDA when available\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_lower_case\",\n",
    "    default=True,\n",
    "    type=bool,\n",
    "    help=\"Whether to lower case the input text. True for uncased models, False for cased models.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local_rank\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"local_rank for distributed training on gpus\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=42, help=\"random seed for initialization\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fp16\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to use 16-bit float precision instead of 32-bit\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loss_scale\",\n",
    "    type=float,\n",
    "    default=0,\n",
    "    help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
    "    \"0 (default value): dynamic loss scaling.\\n\"\n",
    "    \"Positive power of 2: static loss scaling value.\\n\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_workers\",\n",
    "    type=int,\n",
    "    default=16,\n",
    "    help=\"Number of workers in the dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_name\", default=\"\", type=str, help=\"save name for training.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_chunk\",\n",
    "    default=0,\n",
    "    type=float,\n",
    "    help=\"whether use chunck for parallel training.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", default=30, type=int, help=\"what is the batch size?\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tasks\", default=\"\", type=str, help=\"1-2-3... training task separate by -\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--in_memory\",\n",
    "    default=False,\n",
    "    type=bool,\n",
    "    help=\"whether use chunck for parallel training.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--baseline\", action=\"store_true\", help=\"whether use single stream baseline.\"\n",
    ")\n",
    "parser.add_argument(\"--split\", default=\"\", type=str, help=\"which split to use.\")\n",
    "parser.add_argument(\n",
    "    \"--dynamic_attention\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether use dynamic attention.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clean_train_sets\",\n",
    "    default=True,\n",
    "    type=bool,\n",
    "    help=\"whether clean train sets for multitask data.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visual_target\",\n",
    "    default=0,\n",
    "    type=int,\n",
    "    help=\"which target to use for visual branch. \\\n",
    "    0: soft label, \\\n",
    "    1: regress the feature, \\\n",
    "    2: NCE loss.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--task_specific_tokens\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether to use task specific tokens for the multi-task learning.\",\n",
    ")\n",
    "\n",
    "#args = parser.parse_args()\n",
    "args = parser.parse_args(['--bert_model', 'bert-base-uncased',\n",
    "                          '--from_pretrained', 'save/VQA_bert_base_6layer_6conect-finetune_from_multi_task_model/pytorch_model_19.bin',\n",
    "                          '--config_file', 'config/bert_base_6layer_6conect.json',\n",
    "                          '--tasks', '1',\n",
    "                          '--split', 'minval',\n",
    "                          '--save_name', 'testing',\n",
    "                          '--task_specific_tokens'])\n",
    "\n",
    "\n",
    "with open(\"vilbert_tasks.yml\", \"r\") as f:\n",
    "    task_cfg = edict(yaml.safe_load(f))\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.baseline:\n",
    "    from pytorch_transformers.modeling_bert import BertConfig\n",
    "    from vilbert.basebert import BaseBertForVLTasks\n",
    "else:\n",
    "    from vilbert.vilbert import BertConfig\n",
    "    from vilbert.vilbert import VILBertForVLTasks\n",
    "\n",
    "task_names = []\n",
    "for i, task_id in enumerate(args.tasks.split(\"-\")):\n",
    "    task = \"TASK\" + task_id\n",
    "    name = task_cfg[task][\"name\"]\n",
    "    task_names.append(name)\n",
    "\n",
    "# timeStamp = '-'.join(task_names) + '_' + args.config_file.split('/')[1].split('.')[0]\n",
    "timeStamp = args.from_pretrained.split(\"/\")[-1] + \"-\" + args.save_name\n",
    "savePath = os.path.join(args.output_dir, timeStamp)\n",
    "config = BertConfig.from_json_file(args.config_file)\n",
    "\n",
    "if args.task_specific_tokens:\n",
    "    config.task_specific_tokens = True\n",
    "\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\n",
    "        \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "    )\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "\n",
    "logger.info(\n",
    "    \"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16\n",
    "    )\n",
    ")\n",
    "\n",
    "default_gpu = False\n",
    "if dist.is_available() and args.local_rank != -1:\n",
    "    rank = dist.get_rank()\n",
    "    if rank == 0:\n",
    "        default_gpu = True\n",
    "else:\n",
    "    default_gpu = True\n",
    "\n",
    "if default_gpu and not os.path.exists(savePath):\n",
    "    os.makedirs(savePath)\n",
    "\n",
    "task_batch_size, task_num_iters, task_ids, task_datasets_val, task_dataloader_val = LoadDatasetEval(\n",
    "    args, task_cfg, args.tasks.split(\"-\")\n",
    ")\n",
    "\n",
    "tbLogger = utils.tbLogger(\n",
    "    timeStamp,\n",
    "    savePath,\n",
    "    task_names,\n",
    "    task_ids,\n",
    "    task_num_iters,\n",
    "    1,\n",
    "    save_logger=False,\n",
    "    txt_name=\"eval.txt\",\n",
    ")\n",
    "num_labels = max([dataset.num_labels for dataset in task_datasets_val.values()])\n",
    "\n",
    "if args.dynamic_attention:\n",
    "    config.dynamic_attention = True\n",
    "if \"roberta\" in args.bert_model:\n",
    "    config.model = \"roberta\"\n",
    "\n",
    "if args.visual_target == 0:\n",
    "    config.v_target_size = 1601\n",
    "    config.visual_target = args.visual_target\n",
    "else:\n",
    "    config.v_target_size = 2048\n",
    "    config.visual_target = args.visual_target\n",
    "\n",
    "if args.task_specific_tokens:\n",
    "    config.task_specific_tokens = True\n",
    "\n",
    "if args.baseline:\n",
    "    model = BaseBertForVLTasks.from_pretrained(\n",
    "        args.from_pretrained,\n",
    "        config=config,\n",
    "        num_labels=num_labels,\n",
    "        default_gpu=default_gpu,\n",
    "    )\n",
    "else:\n",
    "    model = VILBertForVLTasks.from_pretrained(\n",
    "        args.from_pretrained,\n",
    "        config=config,\n",
    "        num_labels=num_labels,\n",
    "        default_gpu=default_gpu,\n",
    "    )\n",
    "\n",
    "task_losses = LoadLosses(args, task_cfg, args.tasks.split(\"-\"))\n",
    "model.to(device)\n",
    "if args.local_rank != -1:\n",
    "    try:\n",
    "        from apex.parallel import DistributedDataParallel as DDP\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"\n",
    "        )\n",
    "    model = DDP(model, delay_allreduce=True)\n",
    "\n",
    "elif n_gpu > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num Iters: \", task_num_iters)\n",
    "print(\"  Batch size: \", task_batch_size)\n",
    "\n",
    "model.eval()\n",
    "# when run evaluate, we run each task sequentially.\n",
    "for task_id in task_ids:\n",
    "    results = []\n",
    "    others = []\n",
    "    for i, batch in enumerate(task_dataloader_val[task_id]):\n",
    "        loss, score, batch_size, results, others = EvaluatingModel(\n",
    "            args,\n",
    "            task_cfg,\n",
    "            device,\n",
    "            task_id,\n",
    "            batch,\n",
    "            model,\n",
    "            task_dataloader_val,\n",
    "            task_losses,\n",
    "            results,\n",
    "            others,\n",
    "        )\n",
    "\n",
    "        tbLogger.step_val(0, float(loss), float(score), task_id, batch_size, \"val\")\n",
    "\n",
    "        sys.stdout.write(\"%d/%d\\r\" % (i, len(task_dataloader_val[task_id])))\n",
    "        sys.stdout.flush()\n",
    "    # save the result or evaluate the result.\n",
    "    ave_score = tbLogger.showLossVal(task_id)\n",
    "\n",
    "    if args.split:\n",
    "        json_path = os.path.join(savePath, args.split)\n",
    "    else:\n",
    "        json_path = os.path.join(savePath, task_cfg[task_id][\"val_split\"])\n",
    "\n",
    "    json.dump(results, open(json_path + \"_result.json\", \"w\"))\n",
    "    json.dump(others, open(json_path + \"_others.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable              Type                          Data/Info\n",
      "-------------------------------------------------------------\n",
      "BertConfig            type                          <class 'vilbert.vilbert.BertConfig'>\n",
      "EvaluatingModel       function                      <function EvaluatingModel at 0x7fe0f3700b70>\n",
      "F                     module                        <module 'torch.nn.functio<...>/torch/nn/functional.py'>\n",
      "ForwardModelsTrain    function                      <function ForwardModelsTrain at 0x7fe0f37008c8>\n",
      "ForwardModelsVal      function                      <function ForwardModelsVal at 0x7fe0f3700840>\n",
      "LoadDatasetEval       function                      <function LoadDatasetEval at 0x7fe0f3700a60>\n",
      "LoadLosses            function                      <function LoadLosses at 0x7fe0f3700950>\n",
      "SummaryWriter         type                          <class 'tensorboardX.writer.SummaryWriter'>\n",
      "VILBertForVLTasks     type                          <class 'vilbert.vilbert.VILBertForVLTasks'>\n",
      "argparse              module                        <module 'argparse' from '<...>b/python3.6/argparse.py'>\n",
      "args                  Namespace                     Namespace(baseline=False,<...>chunk=0, visual_target=0)\n",
      "ave_score             float                         0.0\n",
      "batch                 list                          n=9\n",
      "batch_size            int                           30\n",
      "bisect                builtin_function_or_method    <built-in function bisect>\n",
      "config                BertConfig                    {\\n  \"attention_probs_dro<...>h_coattention\": true\\n}\\n\n",
      "default_gpu           bool                          True\n",
      "device                device                        cuda\n",
      "dist                  module                        <module 'torch.distribute<...>distributed/__init__.py'>\n",
      "edict                 type                          <class 'easydict.EasyDict'>\n",
      "f                     TextIOWrapper                 <_io.TextIOWrapper name='<...>ode='r' encoding='UTF-8'>\n",
      "i                     int                           99\n",
      "json                  module                        <module 'json' from '/hom<...>hon3.6/json/__init__.py'>\n",
      "json_path             str                           results/pytorch_model_19.bin-testing/minval\n",
      "logger                Logger                        <Logger __main__ (INFO)>\n",
      "logging               module                        <module 'logging' from '/<...>3.6/logging/__init__.py'>\n",
      "loss                  float                         0.0\n",
      "model                 VILBertForVLTasks             VILBertForVLTasks(\\n  (be<...>features=1, bias=True)\\n)\n",
      "n_gpu                 int                           1\n",
      "name                  str                           VQA\n",
      "nn                    module                        <module 'torch.nn' from '<...>es/torch/nn/__init__.py'>\n",
      "np                    module                        <module 'numpy' from '/ho<...>kages/numpy/__init__.py'>\n",
      "num_labels            int                           3129\n",
      "open                  builtin_function_or_method    <built-in function open>\n",
      "os                    module                        <module 'os' from '/home/<...>-mt/lib/python3.6/os.py'>\n",
      "others                list                          n=0\n",
      "parser                ArgumentParser                ArgumentParser(prog='ipyk<...>r='error', add_help=True)\n",
      "pdb                   module                        <module 'pdb' from '/home<...>mt/lib/python3.6/pdb.py'>\n",
      "random                module                        <module 'random' from '/h<...>lib/python3.6/random.py'>\n",
      "results               list                          n=3000\n",
      "savePath              str                           results/pytorch_model_19.bin-testing\n",
      "score                 float                         0.0\n",
      "sys                   module                        <module 'sys' (built-in)>\n",
      "task                  str                           TASK1\n",
      "task_batch_size       dict                          n=1\n",
      "task_cfg              EasyDict                      {'TASK1': {'name': 'VQA',<...> 2e-06, 'num_epoch': 20}}\n",
      "task_dataloader_val   dict                          n=1\n",
      "task_datasets_val     dict                          n=1\n",
      "task_id               str                           TASK1\n",
      "task_ids              list                          n=1\n",
      "task_losses           dict                          n=1\n",
      "task_names            list                          n=1\n",
      "task_num_iters        dict                          n=1\n",
      "tbLogger              tbLogger                      <vilbert.utils.tbLogger object at 0x7fe19a081e48>\n",
      "timeStamp             str                           pytorch_model_19.bin-testing\n",
      "torch                 module                        <module 'torch' from '/ho<...>kages/torch/__init__.py'>\n",
      "tqdm                  type                          <class 'tqdm._tqdm.tqdm'>\n",
      "utils                 module                        <module 'vilbert.utils' f<...>i-task/vilbert/utils.py'>\n",
      "yaml                  module                        <module 'yaml' from '/hom<...>ckages/yaml/__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(baseline=False, batch_size=30, bert_model='bert-base-uncased', clean_train_sets=True, config_file='config/bert_base_6layer_6conect.json', do_lower_case=True, dynamic_attention=False, fp16=False, from_pretrained='save/VQA_bert_base_6layer_6conect-finetune_from_multi_task_model/pytorch_model_19.bin', in_memory=False, local_rank=-1, loss_scale=0, no_cuda=False, num_workers=16, output_dir='results', save_name='testing', seed=42, split='minval', task_specific_tokens=True, tasks='1', use_chunk=0, visual_target=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tuple(t.cuda(device=device, non_blocking=True) for t in batch)\n",
    "\n",
    "features, spatials, image_mask, question, target, input_mask, segment_ids, co_attention_mask, question_id = (\n",
    "    batch\n",
    ")\n",
    "\n",
    "task_tokens = question.new().resize_(question.size(0), 1).fill_(int(task_id[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 3129])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vil_prediction, vil_prediction_gqa, vil_logit, vil_binary_prediction, vil_tri_prediction, vision_prediction, vision_logit, linguisic_prediction, linguisic_logit, _ = model(\n",
    "    question, \n",
    "    features, \n",
    "    spatials, \n",
    "    segment_ids, \n",
    "    input_mask, \n",
    "    image_mask, \n",
    "    co_attention_mask, \n",
    "    task_tokens,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "logits = torch.max(vil_prediction, 1)[1].data  # argmax\n",
    "one_hots = torch.zeros(*target.size()).cuda()\n",
    "one_hots.scatter_(1, logits.view(-1, 1), 1)\n",
    "scores = one_hots * target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7033, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sum() / features.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/aloui/vilbert-multi-task/vilbert/vilbert.py:1333: TracerWarning: resize_ can't be represented in the JIT at the moment, so we won't connect any uses of this value with its current trace. If you happen to use it again, it will show up as a constant in the graph.\n",
      "  mask_tokens = input_txt.new().resize_(input_txt.size(0), 1).fill_(1)\n",
      "/aloui/vilbert-multi-task/vilbert/vilbert.py:1686: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pooled_output.size(0) % 2 == 0:\n",
      "/home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/tensor.py:461: RuntimeWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  'incorrect results).', category=RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracer cannot infer type of (tensor([[-23.9768, -17.3129, -18.0523,  ..., -18.6565, -18.3060, -14.8244],\n",
      "        [-26.2138, -18.8952, -17.7499,  ..., -17.4112, -15.3365, -17.8206],\n",
      "        [-24.8113, -16.7020, -11.6527,  ..., -15.2050, -14.3076, -10.0045],\n",
      "        ...,\n",
      "        [-27.0051, -15.3371, -20.5083,  ..., -19.9755, -16.9174, -17.5259],\n",
      "        [-21.7966, -14.3504, -19.0064,  ..., -13.6335, -11.8910, -26.7599],\n",
      "        [-23.3009, -11.6584, -19.6952,  ..., -18.6055, -13.8711, -18.0167]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-11.6474, -10.8408,  -8.1201,  ..., -13.7666, -13.3638, -14.8178],\n",
      "        [-14.1130,  -8.2416, -11.8985,  ..., -15.3526, -14.5676, -16.1072],\n",
      "        [-10.0281,  -8.5481, -12.1849,  ...,  -9.9922, -13.6915, -11.6269],\n",
      "        ...,\n",
      "        [-20.1335, -18.3920, -17.5234,  ..., -20.9741, -23.5247, -22.5615],\n",
      "        [ -6.5798,  -8.0710,  -7.1975,  ...,  -9.0155,  -8.8422, -10.2870],\n",
      "        [-18.6119, -14.4927, -17.6004,  ..., -18.2182, -18.8035, -19.1235]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-0.6393],\n",
      "        [-0.1429],\n",
      "        [-2.1546],\n",
      "        [-5.1147],\n",
      "        [-1.6715],\n",
      "        [ 0.3507],\n",
      "        [-4.3397],\n",
      "        [ 2.2866],\n",
      "        [-1.4691],\n",
      "        [ 1.4585],\n",
      "        [-0.2973],\n",
      "        [ 0.7474],\n",
      "        [ 2.5556],\n",
      "        [-2.9994],\n",
      "        [-0.9246],\n",
      "        [-1.2214],\n",
      "        [ 0.3344],\n",
      "        [ 2.2141],\n",
      "        [-0.7545],\n",
      "        [ 2.4200],\n",
      "        [-0.0531],\n",
      "        [ 3.1947],\n",
      "        [-0.2689],\n",
      "        [ 0.4529],\n",
      "        [-0.3984],\n",
      "        [-0.2024],\n",
      "        [ 1.1264],\n",
      "        [ 1.3671],\n",
      "        [-3.4349],\n",
      "        [-0.3805]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[ -1.8382,   2.1890],\n",
      "        [  9.7137,  -9.1303],\n",
      "        [  1.1245,  -0.7504],\n",
      "        [ -8.7560,   9.4857],\n",
      "        [-12.7439,  12.7932],\n",
      "        [-12.3451,  12.5709],\n",
      "        [ -1.1877,   0.6844],\n",
      "        [ -4.0932,   4.0128],\n",
      "        [ -4.8093,   5.3705],\n",
      "        [ -3.7177,   3.9173],\n",
      "        [-12.6056,  12.4244],\n",
      "        [ 10.6988, -10.2227],\n",
      "        [  2.8526,  -2.8399],\n",
      "        [ -7.0294,   7.1356],\n",
      "        [-10.3483,   9.9678]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-1.3022, -1.1211, -3.1991],\n",
      "        [-0.0492, -0.8463, -3.6581],\n",
      "        [-0.1244, -1.9052, -2.7197],\n",
      "        [-1.2288,  0.7291, -5.5015],\n",
      "        [-0.3299, -2.6304, -0.9506],\n",
      "        [-1.6865, -1.4229, -2.1603],\n",
      "        [ 0.4084, -1.3018, -4.3268],\n",
      "        [-3.0534, -1.7261,  0.4608],\n",
      "        [ 0.6957, -1.7683, -3.7300],\n",
      "        [-2.5832, -0.6753, -0.2382],\n",
      "        [-0.4761, -1.3521, -2.6981],\n",
      "        [-2.4724, -0.0574, -2.8649],\n",
      "        [-3.2873,  0.2401, -0.6161],\n",
      "        [ 2.8250, -3.0116, -5.6009],\n",
      "        [ 0.4068,  0.0392, -5.6889],\n",
      "        [ 0.2833, -1.2190, -3.7594],\n",
      "        [-1.5270, -1.1284, -2.1290],\n",
      "        [-2.2854, -2.1912,  0.3826],\n",
      "        [-0.8855, -0.2406, -2.5146],\n",
      "        [-2.2929, -1.9686,  0.3879],\n",
      "        [-0.9590, -0.1366, -3.1811],\n",
      "        [-3.6603, -1.3946,  1.0387],\n",
      "        [-2.8627,  0.4189, -2.4454],\n",
      "        [-0.9705, -0.0349, -1.1119],\n",
      "        [-1.2466, -2.2507, -3.0363],\n",
      "        [-0.8483, -2.6603, -3.1504],\n",
      "        [-2.7211, -2.8056, -0.4124],\n",
      "        [-0.7093, -1.7536, -2.3936],\n",
      "        [-1.6274,  0.5352, -3.8358],\n",
      "        [-2.8041, -0.9302, -1.3755]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[[  8.1071,  -5.0885,  -3.7664,  ...,  -1.4273,  -0.9021,  -4.1969],\n",
      "         [  3.8177,  -4.5532,  -2.8071,  ...,  -1.3409,  -1.7692,  -2.2690],\n",
      "         [  7.0173,  -5.5335,  -4.4290,  ...,  -1.3300,   0.1980,  -3.6034],\n",
      "         ...,\n",
      "         [  7.1785,  -6.1633,  -3.6649,  ...,  -1.6600,  -1.7262,  -5.0354],\n",
      "         [  6.9941,  -5.3046,  -2.6913,  ...,  -3.1742,  -2.2862,  -3.5778],\n",
      "         [  7.5086,  -5.9734,  -2.9408,  ...,  -3.7117,  -2.2481,  -3.2648]],\n",
      "\n",
      "        [[  7.4254,  -6.7579,  -4.9328,  ...,  -3.9803,  -2.2647,  -2.4394],\n",
      "         [  7.2897,  -5.9018,  -5.5442,  ...,  -1.9450,  -1.7594,  -4.7357],\n",
      "         [  5.6920,  -5.7744,  -4.4779,  ...,  -3.6136,  -1.5126,  -1.5384],\n",
      "         ...,\n",
      "         [  6.6963,  -6.7992,  -6.3011,  ...,  -3.5846,  -2.3700,  -3.7784],\n",
      "         [  7.2653,  -5.9123,  -5.5892,  ...,  -1.9841,  -1.7899,  -4.7843],\n",
      "         [  7.7138,  -5.0347,  -4.6088,  ...,  -3.1614,  -3.4179,  -4.0131]],\n",
      "\n",
      "        [[  5.2152,  -8.8324,  -3.6273,  ...,  -6.6085,  -4.7241,  -6.8234],\n",
      "         [  4.8676,  -4.3746,  -2.4917,  ...,  -3.3825,  -0.8355,  -2.7390],\n",
      "         [  4.8747,  -7.0232,  -4.1367,  ...,  -4.1859,  -2.7916,  -3.0367],\n",
      "         ...,\n",
      "         [  5.6844, -10.0907,  -4.7995,  ...,  -5.5661,  -3.4763,  -6.9165],\n",
      "         [  3.8252,  -4.4261,  -1.5736,  ...,  -3.6205,  -0.7457,  -2.6781],\n",
      "         [  5.5476,  -8.0797,  -5.3136,  ...,  -5.9175,  -4.6428,  -6.6182]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  6.2329,  -5.6986,  -4.4003,  ...,  -4.9978,  -5.4505,  -6.0065],\n",
      "         [  4.5816,  -3.0084,  -4.3789,  ...,  -3.6890,  -0.4907,  -4.3853],\n",
      "         [  4.2864,  -4.2727,  -3.4970,  ...,  -3.9741,  -3.0780,  -6.3296],\n",
      "         ...,\n",
      "         [  6.5369,  -4.8309,  -2.7178,  ...,  -5.2795,  -4.7246,  -5.6874],\n",
      "         [  7.1938,  -2.4227,  -3.5248,  ...,  -4.8716,  -4.8112,  -6.5379],\n",
      "         [  6.7999,  -4.8870,  -0.2368,  ...,  -5.6579,  -5.1136,  -3.9159]],\n",
      "\n",
      "        [[  4.2967,  -6.3247,  -5.5420,  ...,  -4.6959,  -4.1590,  -5.7608],\n",
      "         [  5.8049,  -3.6484,  -4.5866,  ...,  -3.8342,  -1.8794,  -5.5023],\n",
      "         [  5.3276,  -3.8784,  -3.4268,  ...,  -3.0494,  -3.3516,  -5.3999],\n",
      "         ...,\n",
      "         [  4.5262,  -5.7328,  -4.5576,  ...,  -5.2571,  -4.0224,  -4.7971],\n",
      "         [  6.1058,  -2.9771,  -4.3419,  ...,  -3.4850,  -3.5584,  -6.0711],\n",
      "         [  6.0065,  -6.8486,  -2.9187,  ...,  -5.1517,  -3.8328,  -5.4005]],\n",
      "\n",
      "        [[  6.7952,  -5.9581,  -3.8170,  ...,  -6.1306,  -3.8980,  -5.2182],\n",
      "         [  4.7843,  -4.3625,  -3.4503,  ...,  -3.0429,  -1.0069,  -5.0160],\n",
      "         [  4.7423,  -4.0734,  -2.0026,  ...,  -3.3397,  -1.5319,  -5.5408],\n",
      "         ...,\n",
      "         [  5.9233,  -6.7720,  -3.2585,  ...,  -5.0303,  -3.6403,  -6.4752],\n",
      "         [  6.4932,  -3.8687,  -2.0943,  ...,  -4.3999,  -3.3642,  -6.5363],\n",
      "         [  7.4559,  -5.1406,   0.5284,  ...,  -4.9604,  -4.5710,  -5.3826]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-1.0254],\n",
      "         [ 0.5791],\n",
      "         [-0.6458],\n",
      "         ...,\n",
      "         [-1.2198],\n",
      "         [-2.7965],\n",
      "         [-1.1531]],\n",
      "\n",
      "        [[-1.1604],\n",
      "         [-0.9972],\n",
      "         [-2.1179],\n",
      "         ...,\n",
      "         [-0.9099],\n",
      "         [-1.0286],\n",
      "         [-3.0328]],\n",
      "\n",
      "        [[-5.0549],\n",
      "         [-3.8171],\n",
      "         [-1.7807],\n",
      "         ...,\n",
      "         [-5.8512],\n",
      "         [-3.3489],\n",
      "         [-4.7341]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.6947],\n",
      "         [-0.4835],\n",
      "         [-4.9195],\n",
      "         ...,\n",
      "         [-3.5398],\n",
      "         [-3.4552],\n",
      "         [-3.4297]],\n",
      "\n",
      "        [[ 0.4876],\n",
      "         [-2.0698],\n",
      "         [-3.0662],\n",
      "         ...,\n",
      "         [-1.2132],\n",
      "         [-1.7824],\n",
      "         [-0.7635]],\n",
      "\n",
      "        [[ 0.7829],\n",
      "         [ 0.2189],\n",
      "         [-2.1548],\n",
      "         ...,\n",
      "         [-1.7861],\n",
      "         [-2.7979],\n",
      "         [-1.9784]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-3.6498, -3.4908, -3.6187,  ..., -3.5289, -3.8178, -3.6744],\n",
      "         [-3.2804, -3.1006, -3.1543,  ..., -3.0870, -3.3823, -3.2175],\n",
      "         [-3.5100, -3.3237, -3.3740,  ..., -3.3072, -3.5707, -3.4169],\n",
      "         ...,\n",
      "         [-4.1081, -4.0421, -4.0378,  ..., -3.9809, -4.1430, -4.1447],\n",
      "         [-4.1650, -4.0987, -4.1115,  ..., -4.0453, -4.2243, -4.2235],\n",
      "         [-4.1392, -4.0736, -4.0614,  ..., -3.9937, -4.1944, -4.1707]],\n",
      "\n",
      "        [[-2.3112, -2.3721, -2.2133,  ..., -2.6004, -2.6427, -2.4511],\n",
      "         [-3.4823, -3.5827, -3.4849,  ..., -3.5824, -3.7505, -3.5713],\n",
      "         [-3.5046, -3.6621, -3.5683,  ..., -3.6990, -3.8633, -3.6441],\n",
      "         ...,\n",
      "         [-3.8630, -3.8739, -3.7105,  ..., -4.0762, -4.0627, -3.8866],\n",
      "         [-3.8149, -3.8647, -3.6870,  ..., -4.0637, -4.0323, -3.8605],\n",
      "         [-4.1886, -4.2119, -4.0773,  ..., -4.4352, -4.3988, -4.2430]],\n",
      "\n",
      "        [[-1.9862, -1.7931, -1.7450,  ..., -1.8751, -2.1345, -2.0390],\n",
      "         [-1.5100, -1.3702, -1.2807,  ..., -1.4770, -1.6623, -1.6035],\n",
      "         [-1.5638, -1.4044, -1.3180,  ..., -1.5494, -1.7191, -1.6515],\n",
      "         ...,\n",
      "         [-1.8217, -1.6298, -1.5178,  ..., -1.8839, -1.9941, -1.8687],\n",
      "         [-2.0018, -1.8120, -1.7015,  ..., -2.0700, -2.1824, -2.0502],\n",
      "         [-1.9469, -1.7752, -1.6727,  ..., -2.0368, -2.1384, -1.9946]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7459, -0.2746, -0.5147,  ..., -0.6378, -0.9469, -0.7759],\n",
      "         [-0.9958, -0.5589, -0.7966,  ..., -0.9018, -1.2208, -1.0370],\n",
      "         [-1.0953, -0.6611, -0.8872,  ..., -0.9099, -1.3346, -1.0890],\n",
      "         ...,\n",
      "         [-1.7204, -1.1448, -1.5489,  ..., -1.9175, -1.9201, -1.9237],\n",
      "         [-1.1636, -0.6134, -0.9931,  ..., -1.3438, -1.3703, -1.3533],\n",
      "         [-1.0375, -0.4929, -0.8435,  ..., -1.2396, -1.2365, -1.2116]],\n",
      "\n",
      "        [[-2.3704, -2.1079, -2.2421,  ..., -2.4508, -2.8402, -2.6562],\n",
      "         [-3.5595, -3.2445, -3.5233,  ..., -3.5955, -3.9896, -3.7924],\n",
      "         [-4.2090, -3.8361, -4.1596,  ..., -4.1861, -4.6080, -4.4536],\n",
      "         ...,\n",
      "         [-2.9149, -2.6139, -2.8405,  ..., -3.0460, -3.2139, -3.1900],\n",
      "         [-2.9831, -2.6284, -2.8564,  ..., -3.0792, -3.2402, -3.2508],\n",
      "         [-2.7952, -2.4774, -2.7095,  ..., -2.9133, -3.0668, -3.0817]],\n",
      "\n",
      "        [[-2.1046, -1.7263, -1.9545,  ..., -2.2074, -2.3714, -2.0817],\n",
      "         [-2.8306, -2.4761, -2.6339,  ..., -2.8985, -3.0506, -2.7840],\n",
      "         [-2.7562, -2.3964, -2.5503,  ..., -2.8107, -2.9543, -2.6950],\n",
      "         ...,\n",
      "         [-2.1971, -1.8356, -2.0023,  ..., -2.3088, -2.3486, -2.1684],\n",
      "         [-2.1272, -1.7860, -1.9482,  ..., -2.2151, -2.2926, -2.0856],\n",
      "         [-1.8418, -1.4547, -1.6640,  ..., -1.9770, -1.9930, -1.8990]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[ 5.4066e-02],\n",
      "         [ 6.5631e-02],\n",
      "         [ 7.8555e-02],\n",
      "         [ 8.2301e-02],\n",
      "         [ 5.5858e-02],\n",
      "         [ 1.7058e-02],\n",
      "         [ 5.6861e-02],\n",
      "         [ 4.3544e-02],\n",
      "         [ 5.9106e-02],\n",
      "         [ 1.0269e-01],\n",
      "         [ 6.7681e-02],\n",
      "         [ 5.7432e-02],\n",
      "         [ 4.9514e-02],\n",
      "         [ 4.2039e-02],\n",
      "         [ 4.9348e-02],\n",
      "         [ 9.6293e-02],\n",
      "         [ 2.2876e-02],\n",
      "         [ 5.0893e-02],\n",
      "         [ 5.4959e-02],\n",
      "         [ 3.7836e-02],\n",
      "         [ 5.6841e-02],\n",
      "         [ 5.0191e-02],\n",
      "         [ 6.1113e-02],\n",
      "         [ 9.0875e-02]],\n",
      "\n",
      "        [[-6.7664e-02],\n",
      "         [-1.6427e-01],\n",
      "         [-1.6623e-01],\n",
      "         [-1.2173e-01],\n",
      "         [-1.8003e-01],\n",
      "         [-1.6219e-01],\n",
      "         [-1.0723e-02],\n",
      "         [-1.5421e-01],\n",
      "         [-4.9339e-02],\n",
      "         [-3.8281e-02],\n",
      "         [-1.8616e-01],\n",
      "         [ 2.1373e-01],\n",
      "         [-8.6579e-02],\n",
      "         [-9.1772e-02],\n",
      "         [-1.0777e-01],\n",
      "         [-1.2135e-01],\n",
      "         [-8.8904e-02],\n",
      "         [-1.5278e-01],\n",
      "         [-1.2438e-01],\n",
      "         [-1.0036e-01],\n",
      "         [-1.1380e-01],\n",
      "         [-9.2139e-02],\n",
      "         [-8.9316e-02],\n",
      "         [-1.1768e-01]],\n",
      "\n",
      "        [[ 6.5625e-02],\n",
      "         [ 6.0019e-02],\n",
      "         [ 5.6963e-02],\n",
      "         [ 4.7871e-02],\n",
      "         [ 1.7319e-02],\n",
      "         [ 4.6699e-02],\n",
      "         [-1.6525e-01],\n",
      "         [ 6.3119e-02],\n",
      "         [-1.7403e-02],\n",
      "         [ 8.5018e-02],\n",
      "         [ 1.1315e-01],\n",
      "         [ 1.1513e-01],\n",
      "         [ 1.0047e-01],\n",
      "         [ 9.8939e-02],\n",
      "         [ 7.9134e-02],\n",
      "         [ 1.0424e-01],\n",
      "         [ 9.2704e-02],\n",
      "         [ 1.2133e-01],\n",
      "         [ 9.9248e-02],\n",
      "         [ 4.4340e-02],\n",
      "         [ 8.7588e-02],\n",
      "         [ 1.3086e-01],\n",
      "         [ 1.1863e-01],\n",
      "         [ 9.6424e-02]],\n",
      "\n",
      "        [[ 5.2794e-02],\n",
      "         [ 1.0923e-01],\n",
      "         [ 1.1409e-01],\n",
      "         [ 7.6880e-02],\n",
      "         [ 5.6569e-02],\n",
      "         [ 6.6162e-02],\n",
      "         [ 1.1882e-01],\n",
      "         [-7.2202e-02],\n",
      "         [ 9.9407e-02],\n",
      "         [ 1.9248e-01],\n",
      "         [ 1.1154e-01],\n",
      "         [ 1.1839e-01],\n",
      "         [ 1.3504e-01],\n",
      "         [ 1.4178e-01],\n",
      "         [ 1.5760e-01],\n",
      "         [ 1.1420e-01],\n",
      "         [ 1.0585e-01],\n",
      "         [ 1.4212e-01],\n",
      "         [ 1.2363e-01],\n",
      "         [ 7.6701e-02],\n",
      "         [ 1.2803e-01],\n",
      "         [ 1.4405e-01],\n",
      "         [ 1.5990e-01],\n",
      "         [ 1.7584e-01]],\n",
      "\n",
      "        [[-1.7807e-01],\n",
      "         [-2.6471e-01],\n",
      "         [-2.6448e-01],\n",
      "         [-2.6246e-01],\n",
      "         [-1.7450e-01],\n",
      "         [-1.4318e-01],\n",
      "         [-2.0536e-01],\n",
      "         [-2.7054e-01],\n",
      "         [-2.6496e-01],\n",
      "         [ 2.3996e-02],\n",
      "         [-1.9643e-01],\n",
      "         [-1.8517e-01],\n",
      "         [-1.5990e-01],\n",
      "         [-1.4904e-01],\n",
      "         [-1.7138e-01],\n",
      "         [-1.4573e-01],\n",
      "         [-1.5216e-01],\n",
      "         [-1.3528e-01],\n",
      "         [-1.4958e-01],\n",
      "         [-1.6462e-01],\n",
      "         [-1.6598e-01],\n",
      "         [-2.1219e-01],\n",
      "         [-2.1703e-01],\n",
      "         [-1.9016e-01]],\n",
      "\n",
      "        [[-8.1026e-02],\n",
      "         [ 1.5849e-02],\n",
      "         [ 4.3204e-02],\n",
      "         [ 4.3463e-02],\n",
      "         [-9.3584e-03],\n",
      "         [ 5.5187e-03],\n",
      "         [ 3.1684e-02],\n",
      "         [ 3.5463e-02],\n",
      "         [ 3.6369e-02],\n",
      "         [ 1.6242e-02],\n",
      "         [ 2.1057e-01],\n",
      "         [ 6.2618e-02],\n",
      "         [ 4.8180e-02],\n",
      "         [ 1.1223e-01],\n",
      "         [ 8.8505e-02],\n",
      "         [ 9.5567e-02],\n",
      "         [ 6.4552e-02],\n",
      "         [ 3.1516e-02],\n",
      "         [ 1.1724e-01],\n",
      "         [ 7.3821e-02],\n",
      "         [ 4.7713e-02],\n",
      "         [ 6.2399e-02],\n",
      "         [ 7.8784e-02],\n",
      "         [ 7.6747e-02]],\n",
      "\n",
      "        [[-1.1578e-01],\n",
      "         [-1.4448e-01],\n",
      "         [-1.2108e-01],\n",
      "         [-1.1716e-01],\n",
      "         [-1.3413e-01],\n",
      "         [-6.0049e-02],\n",
      "         [-1.0755e-01],\n",
      "         [-1.5272e-01],\n",
      "         [-1.4546e-01],\n",
      "         [-2.6533e-01],\n",
      "         [-1.5269e-01],\n",
      "         [ 1.7279e-01],\n",
      "         [-1.9391e-01],\n",
      "         [-8.6409e-02],\n",
      "         [-1.7555e-01],\n",
      "         [-1.7146e-01],\n",
      "         [-1.5573e-01],\n",
      "         [-1.4953e-01],\n",
      "         [-2.7697e-02],\n",
      "         [-1.7865e-01],\n",
      "         [-1.1629e-01],\n",
      "         [-6.9994e-02],\n",
      "         [-9.2495e-02],\n",
      "         [-1.9928e-01]],\n",
      "\n",
      "        [[ 3.9459e-02],\n",
      "         [-5.4832e-02],\n",
      "         [-6.4811e-02],\n",
      "         [-8.4612e-02],\n",
      "         [-2.3616e-02],\n",
      "         [-5.4199e-02],\n",
      "         [-7.1516e-02],\n",
      "         [-4.1795e-02],\n",
      "         [-6.5328e-02],\n",
      "         [ 2.9066e-01],\n",
      "         [ 1.4585e-01],\n",
      "         [ 1.6468e-01],\n",
      "         [ 1.3082e-01],\n",
      "         [ 1.3881e-01],\n",
      "         [ 1.1905e-01],\n",
      "         [ 1.1631e-01],\n",
      "         [ 1.1235e-01],\n",
      "         [ 9.4683e-02],\n",
      "         [ 1.6876e-01],\n",
      "         [ 1.5931e-01],\n",
      "         [ 1.6417e-01],\n",
      "         [ 1.6234e-01],\n",
      "         [ 1.1294e-01],\n",
      "         [ 1.8581e-01]],\n",
      "\n",
      "        [[-6.7158e-02],\n",
      "         [-6.1223e-02],\n",
      "         [-8.2407e-02],\n",
      "         [-6.6869e-02],\n",
      "         [-5.5211e-02],\n",
      "         [ 1.2791e-02],\n",
      "         [-6.7059e-02],\n",
      "         [ 1.9043e-01],\n",
      "         [ 1.2470e-01],\n",
      "         [ 1.0622e-01],\n",
      "         [ 1.1262e-01],\n",
      "         [ 1.1784e-01],\n",
      "         [ 1.2425e-01],\n",
      "         [ 1.4761e-01],\n",
      "         [ 1.1366e-01],\n",
      "         [ 1.2769e-01],\n",
      "         [ 1.8647e-01],\n",
      "         [ 1.6956e-01],\n",
      "         [ 1.0577e-01],\n",
      "         [ 1.8000e-01],\n",
      "         [ 1.7869e-01],\n",
      "         [ 1.5395e-01],\n",
      "         [ 1.2786e-01],\n",
      "         [ 1.0609e-01]],\n",
      "\n",
      "        [[-8.6342e-02],\n",
      "         [-1.2166e-01],\n",
      "         [-2.7074e-01],\n",
      "         [-1.4543e-01],\n",
      "         [-1.5476e-01],\n",
      "         [-1.1159e-01],\n",
      "         [-8.4526e-02],\n",
      "         [-2.3460e-02],\n",
      "         [-5.5933e-02],\n",
      "         [-3.3472e-02],\n",
      "         [-1.7022e-03],\n",
      "         [-6.1379e-02],\n",
      "         [-4.8252e-02],\n",
      "         [-3.3536e-02],\n",
      "         [-4.7247e-02],\n",
      "         [ 6.0844e-02],\n",
      "         [ 3.9172e-02],\n",
      "         [-1.0941e-01],\n",
      "         [ 3.3746e-02],\n",
      "         [ 4.2120e-02],\n",
      "         [ 7.4103e-02],\n",
      "         [ 6.7480e-02],\n",
      "         [-3.9778e-02],\n",
      "         [ 4.6653e-02]],\n",
      "\n",
      "        [[ 1.1761e-04],\n",
      "         [-3.2319e-02],\n",
      "         [-8.2931e-02],\n",
      "         [-6.2712e-02],\n",
      "         [-5.5349e-02],\n",
      "         [-1.6789e-01],\n",
      "         [-5.7528e-02],\n",
      "         [-3.3631e-02],\n",
      "         [-8.9793e-02],\n",
      "         [-4.4528e-02],\n",
      "         [ 1.9845e-01],\n",
      "         [ 4.1326e-02],\n",
      "         [ 6.2142e-02],\n",
      "         [ 5.2120e-02],\n",
      "         [ 4.5284e-02],\n",
      "         [ 5.3875e-03],\n",
      "         [ 2.3621e-02],\n",
      "         [ 4.3163e-02],\n",
      "         [ 2.7918e-02],\n",
      "         [-2.7312e-03],\n",
      "         [ 2.5573e-02],\n",
      "         [ 2.2228e-02],\n",
      "         [ 2.3738e-02],\n",
      "         [ 4.5626e-02]],\n",
      "\n",
      "        [[ 1.1152e-01],\n",
      "         [ 1.1857e-01],\n",
      "         [ 1.2849e-01],\n",
      "         [ 1.0917e-01],\n",
      "         [ 9.5948e-02],\n",
      "         [ 5.3384e-02],\n",
      "         [ 1.0204e-03],\n",
      "         [ 1.0989e-01],\n",
      "         [ 6.0001e-02],\n",
      "         [ 1.5550e-01],\n",
      "         [ 1.5263e-01],\n",
      "         [ 1.9080e-01],\n",
      "         [ 1.5649e-01],\n",
      "         [ 1.7876e-01],\n",
      "         [ 1.4794e-01],\n",
      "         [ 1.8362e-01],\n",
      "         [ 3.1785e-01],\n",
      "         [ 1.4816e-01],\n",
      "         [ 1.9398e-01],\n",
      "         [ 2.9620e-01],\n",
      "         [ 1.6768e-01],\n",
      "         [ 2.6444e-01],\n",
      "         [ 1.7876e-01],\n",
      "         [ 1.6529e-01]],\n",
      "\n",
      "        [[-3.2135e-01],\n",
      "         [-2.7609e-01],\n",
      "         [-2.2708e-01],\n",
      "         [-2.7871e-01],\n",
      "         [-1.5976e-01],\n",
      "         [-1.0789e-01],\n",
      "         [-2.6115e-01],\n",
      "         [ 2.4042e-01],\n",
      "         [ 1.7649e-01],\n",
      "         [ 1.0470e-01],\n",
      "         [ 7.6834e-02],\n",
      "         [ 1.1821e-01],\n",
      "         [ 9.2225e-02],\n",
      "         [ 1.8485e-01],\n",
      "         [ 1.6472e-01],\n",
      "         [ 1.1662e-01],\n",
      "         [ 1.3644e-01],\n",
      "         [ 1.0456e-01],\n",
      "         [ 1.4690e-01],\n",
      "         [ 1.0390e-01],\n",
      "         [ 1.6453e-01],\n",
      "         [ 1.6165e-01],\n",
      "         [ 1.5546e-01],\n",
      "         [ 1.7893e-01]],\n",
      "\n",
      "        [[ 2.2485e-01],\n",
      "         [ 1.5733e-01],\n",
      "         [ 1.8498e-01],\n",
      "         [ 1.1078e-01],\n",
      "         [ 2.0727e-01],\n",
      "         [ 1.8171e-01],\n",
      "         [ 4.9812e-02],\n",
      "         [ 1.6734e-02],\n",
      "         [ 1.8851e-01],\n",
      "         [ 2.8564e-01],\n",
      "         [ 2.0382e-01],\n",
      "         [ 1.7996e-01],\n",
      "         [ 1.8341e-01],\n",
      "         [ 1.6929e-01],\n",
      "         [ 1.5521e-01],\n",
      "         [ 1.3484e-01],\n",
      "         [ 1.5028e-01],\n",
      "         [ 1.4007e-01],\n",
      "         [ 9.8987e-02],\n",
      "         [ 1.9394e-01],\n",
      "         [ 2.3100e-01],\n",
      "         [ 2.1777e-01],\n",
      "         [ 1.4207e-01],\n",
      "         [ 3.0390e-01]],\n",
      "\n",
      "        [[-2.8275e-01],\n",
      "         [-1.7993e-01],\n",
      "         [-1.3902e-01],\n",
      "         [-1.6490e-01],\n",
      "         [-1.0496e-01],\n",
      "         [-2.0422e-01],\n",
      "         [-1.8531e-01],\n",
      "         [-1.7398e-01],\n",
      "         [-2.2590e-01],\n",
      "         [-1.9143e-01],\n",
      "         [ 3.7845e-01],\n",
      "         [-1.4968e-01],\n",
      "         [-1.7335e-01],\n",
      "         [-2.0042e-01],\n",
      "         [-1.4693e-01],\n",
      "         [-1.1373e-01],\n",
      "         [-4.5550e-02],\n",
      "         [-1.5862e-01],\n",
      "         [-1.9468e-01],\n",
      "         [-9.1910e-02],\n",
      "         [-2.0067e-01],\n",
      "         [-1.8242e-01],\n",
      "         [-2.3379e-01],\n",
      "         [-2.4639e-01]],\n",
      "\n",
      "        [[ 3.5746e-02],\n",
      "         [ 1.1198e-01],\n",
      "         [ 1.5230e-01],\n",
      "         [ 2.2968e-01],\n",
      "         [ 1.2334e-01],\n",
      "         [ 7.9760e-02],\n",
      "         [ 2.4281e-02],\n",
      "         [-1.0909e-02],\n",
      "         [ 1.1329e-01],\n",
      "         [ 2.1473e-01],\n",
      "         [ 1.6258e-01],\n",
      "         [ 1.7210e-01],\n",
      "         [ 1.3568e-01],\n",
      "         [ 9.7467e-02],\n",
      "         [ 8.7748e-02],\n",
      "         [ 1.7078e-01],\n",
      "         [ 1.8598e-01],\n",
      "         [ 8.9025e-02],\n",
      "         [ 1.1402e-01],\n",
      "         [ 1.6093e-01],\n",
      "         [ 2.7516e-01],\n",
      "         [ 1.6528e-01],\n",
      "         [ 1.5849e-01],\n",
      "         [ 1.4951e-01]],\n",
      "\n",
      "        [[-6.7726e-02],\n",
      "         [-2.6111e-02],\n",
      "         [-3.7656e-02],\n",
      "         [-3.9180e-02],\n",
      "         [-2.5971e-02],\n",
      "         [-1.4467e-01],\n",
      "         [-1.0697e-02],\n",
      "         [-1.7789e-02],\n",
      "         [-8.8801e-02],\n",
      "         [-1.5108e-01],\n",
      "         [-3.3329e-02],\n",
      "         [ 2.3654e-01],\n",
      "         [ 2.0878e-02],\n",
      "         [ 2.2508e-02],\n",
      "         [ 2.8622e-02],\n",
      "         [-1.1708e-02],\n",
      "         [-4.0135e-02],\n",
      "         [ 2.9418e-02],\n",
      "         [-9.4731e-03],\n",
      "         [-2.9228e-03],\n",
      "         [-2.7782e-02],\n",
      "         [ 2.2871e-02],\n",
      "         [ 1.7818e-02],\n",
      "         [ 2.9181e-02]],\n",
      "\n",
      "        [[ 4.7783e-02],\n",
      "         [ 8.7086e-02],\n",
      "         [ 7.9555e-02],\n",
      "         [ 6.4638e-02],\n",
      "         [ 1.0188e-02],\n",
      "         [-7.4722e-03],\n",
      "         [ 7.2756e-02],\n",
      "         [ 7.3916e-02],\n",
      "         [ 7.0337e-02],\n",
      "         [ 1.6638e-01],\n",
      "         [ 9.5175e-02],\n",
      "         [ 8.1817e-02],\n",
      "         [ 1.5183e-01],\n",
      "         [ 7.3419e-02],\n",
      "         [ 1.0479e-01],\n",
      "         [ 8.2887e-02],\n",
      "         [ 6.3357e-02],\n",
      "         [ 1.0268e-01],\n",
      "         [ 1.0171e-01],\n",
      "         [ 7.6174e-02],\n",
      "         [ 8.9681e-02],\n",
      "         [ 9.0263e-02],\n",
      "         [ 7.2253e-02],\n",
      "         [ 4.0903e-02]],\n",
      "\n",
      "        [[-1.8114e-02],\n",
      "         [-3.1521e-02],\n",
      "         [-3.0011e-02],\n",
      "         [-3.3700e-02],\n",
      "         [-3.9190e-02],\n",
      "         [-7.1181e-02],\n",
      "         [-1.3108e-01],\n",
      "         [-2.4210e-02],\n",
      "         [-3.9979e-02],\n",
      "         [-5.0791e-02],\n",
      "         [-7.0238e-03],\n",
      "         [-4.6672e-02],\n",
      "         [ 3.8189e-01],\n",
      "         [-3.8349e-02],\n",
      "         [-8.6204e-02],\n",
      "         [-3.9688e-02],\n",
      "         [ 1.9305e-02],\n",
      "         [-6.7339e-02],\n",
      "         [-3.3100e-02],\n",
      "         [ 5.0904e-03],\n",
      "         [-1.5047e-02],\n",
      "         [-4.9961e-04],\n",
      "         [-1.9295e-02],\n",
      "         [-1.3981e-02]],\n",
      "\n",
      "        [[ 1.4099e-02],\n",
      "         [ 6.8466e-02],\n",
      "         [ 7.5729e-02],\n",
      "         [ 6.5672e-02],\n",
      "         [ 5.7519e-02],\n",
      "         [ 1.3579e-03],\n",
      "         [ 9.5770e-04],\n",
      "         [ 1.6213e-02],\n",
      "         [ 1.4458e-02],\n",
      "         [ 9.2938e-04],\n",
      "         [ 7.3183e-04],\n",
      "         [ 2.2929e-02],\n",
      "         [ 1.6425e-01],\n",
      "         [ 1.3149e-01],\n",
      "         [ 1.3060e-01],\n",
      "         [ 1.2997e-01],\n",
      "         [ 1.4727e-01],\n",
      "         [ 1.5043e-01],\n",
      "         [ 1.4297e-01],\n",
      "         [ 1.5185e-01],\n",
      "         [ 1.2384e-01],\n",
      "         [ 1.8071e-01],\n",
      "         [ 9.4006e-02],\n",
      "         [ 1.1730e-01]],\n",
      "\n",
      "        [[-2.9610e-01],\n",
      "         [-2.3449e-01],\n",
      "         [-2.2987e-01],\n",
      "         [-2.7641e-01],\n",
      "         [-2.4688e-01],\n",
      "         [-3.2661e-01],\n",
      "         [-2.6682e-01],\n",
      "         [-2.5042e-01],\n",
      "         [-3.3929e-01],\n",
      "         [-4.5027e-01],\n",
      "         [-2.1687e-01],\n",
      "         [-2.9021e-01],\n",
      "         [-2.4233e-01],\n",
      "         [ 2.8537e-01],\n",
      "         [-2.8036e-01],\n",
      "         [-2.9515e-01],\n",
      "         [-3.0914e-01],\n",
      "         [-3.1012e-01],\n",
      "         [-3.1859e-01],\n",
      "         [-3.0750e-01],\n",
      "         [-3.0444e-01],\n",
      "         [-2.5483e-01],\n",
      "         [-2.9749e-01],\n",
      "         [-2.7640e-01]],\n",
      "\n",
      "        [[-5.2741e-01],\n",
      "         [-4.1537e-01],\n",
      "         [-3.7248e-01],\n",
      "         [-3.3458e-01],\n",
      "         [-2.7509e-01],\n",
      "         [-3.5539e-01],\n",
      "         [-4.7727e-01],\n",
      "         [-7.2280e-01],\n",
      "         [-6.3663e-01],\n",
      "         [-6.8911e-01],\n",
      "         [-5.5427e-01],\n",
      "         [-3.6727e-01],\n",
      "         [ 1.5880e-01],\n",
      "         [-5.3511e-01],\n",
      "         [-5.0268e-01],\n",
      "         [-5.0363e-01],\n",
      "         [-3.7419e-01],\n",
      "         [-3.0913e-01],\n",
      "         [-5.0107e-01],\n",
      "         [-3.5062e-01],\n",
      "         [-5.1421e-01],\n",
      "         [-5.7398e-01],\n",
      "         [-7.1038e-01],\n",
      "         [-5.1138e-01]],\n",
      "\n",
      "        [[-4.6731e-01],\n",
      "         [-4.7546e-01],\n",
      "         [-4.7340e-01],\n",
      "         [-5.3868e-01],\n",
      "         [-4.6113e-01],\n",
      "         [-4.7867e-01],\n",
      "         [-5.9746e-01],\n",
      "         [-5.4122e-01],\n",
      "         [-5.6321e-01],\n",
      "         [-4.7033e-01],\n",
      "         [-4.6819e-01],\n",
      "         [ 7.6297e-02],\n",
      "         [-4.5826e-01],\n",
      "         [-4.3651e-01],\n",
      "         [-4.4497e-01],\n",
      "         [-4.8697e-01],\n",
      "         [-4.5469e-01],\n",
      "         [-4.2812e-01],\n",
      "         [-4.5190e-01],\n",
      "         [-4.4646e-01],\n",
      "         [-4.7458e-01],\n",
      "         [-4.2964e-01],\n",
      "         [-4.4461e-01],\n",
      "         [-4.3520e-01]],\n",
      "\n",
      "        [[ 2.3901e-02],\n",
      "         [-8.3929e-02],\n",
      "         [-5.9767e-02],\n",
      "         [-7.8982e-02],\n",
      "         [-9.1446e-02],\n",
      "         [-7.5392e-02],\n",
      "         [-8.0575e-02],\n",
      "         [-1.6147e-01],\n",
      "         [-1.3367e-01],\n",
      "         [-1.0356e-01],\n",
      "         [ 2.7246e-01],\n",
      "         [-6.2741e-02],\n",
      "         [-8.1526e-02],\n",
      "         [-8.0032e-02],\n",
      "         [-1.0672e-01],\n",
      "         [-1.1291e-01],\n",
      "         [-1.1773e-01],\n",
      "         [-1.0829e-01],\n",
      "         [-9.8497e-02],\n",
      "         [-8.1571e-02],\n",
      "         [-8.9528e-02],\n",
      "         [-5.8892e-02],\n",
      "         [-6.5702e-02],\n",
      "         [-6.0051e-02]],\n",
      "\n",
      "        [[ 4.3231e-01],\n",
      "         [ 3.1132e-01],\n",
      "         [ 3.0673e-01],\n",
      "         [ 3.0883e-01],\n",
      "         [ 3.2686e-01],\n",
      "         [ 4.0427e-01],\n",
      "         [ 2.9928e-01],\n",
      "         [ 2.4772e-01],\n",
      "         [ 2.8525e-01],\n",
      "         [ 2.3264e-01],\n",
      "         [ 3.2084e-01],\n",
      "         [ 4.6697e-01],\n",
      "         [ 3.0005e-01],\n",
      "         [ 3.2966e-01],\n",
      "         [ 3.1251e-01],\n",
      "         [ 2.9748e-01],\n",
      "         [ 2.9741e-01],\n",
      "         [ 3.1698e-01],\n",
      "         [ 3.3682e-01],\n",
      "         [ 3.0614e-01],\n",
      "         [ 2.9851e-01],\n",
      "         [ 3.3857e-01],\n",
      "         [ 3.3640e-01],\n",
      "         [ 3.2906e-01]],\n",
      "\n",
      "        [[ 3.7814e-01],\n",
      "         [ 2.7629e-01],\n",
      "         [ 2.7702e-01],\n",
      "         [ 2.7938e-01],\n",
      "         [ 3.0291e-01],\n",
      "         [ 4.2052e-01],\n",
      "         [ 3.9039e-01],\n",
      "         [ 2.7374e-01],\n",
      "         [ 2.7679e-01],\n",
      "         [ 2.8592e-01],\n",
      "         [ 3.5162e-01],\n",
      "         [ 3.4790e-01],\n",
      "         [ 3.0891e-01],\n",
      "         [ 3.0513e-01],\n",
      "         [ 3.5150e-01],\n",
      "         [ 3.5958e-01],\n",
      "         [ 4.0797e-01],\n",
      "         [ 3.0780e-01],\n",
      "         [ 3.3473e-01],\n",
      "         [ 4.5103e-01],\n",
      "         [ 3.3080e-01],\n",
      "         [ 3.7032e-01],\n",
      "         [ 3.1657e-01],\n",
      "         [ 3.1476e-01]],\n",
      "\n",
      "        [[ 2.6813e-01],\n",
      "         [ 2.0210e-01],\n",
      "         [ 2.1139e-01],\n",
      "         [ 1.7551e-01],\n",
      "         [ 1.8097e-01],\n",
      "         [ 1.8400e-01],\n",
      "         [ 1.7217e-01],\n",
      "         [ 1.8994e-01],\n",
      "         [ 4.3222e-01],\n",
      "         [ 2.5479e-01],\n",
      "         [ 2.0056e-01],\n",
      "         [ 2.4933e-01],\n",
      "         [ 2.1955e-01],\n",
      "         [ 2.4208e-01],\n",
      "         [ 1.8911e-01],\n",
      "         [ 1.9604e-01],\n",
      "         [ 1.7348e-01],\n",
      "         [ 2.9323e-01],\n",
      "         [ 2.9433e-01],\n",
      "         [ 2.2136e-01],\n",
      "         [ 3.0027e-01],\n",
      "         [ 3.0404e-01],\n",
      "         [ 3.4177e-01],\n",
      "         [ 3.1198e-01]],\n",
      "\n",
      "        [[ 3.2656e-01],\n",
      "         [ 2.8771e-01],\n",
      "         [ 2.9052e-01],\n",
      "         [ 2.7728e-01],\n",
      "         [ 4.5873e-01],\n",
      "         [ 2.4484e-01],\n",
      "         [ 2.7377e-01],\n",
      "         [ 4.9087e-01],\n",
      "         [ 3.1983e-01],\n",
      "         [ 2.7421e-01],\n",
      "         [ 2.1840e-01],\n",
      "         [ 3.6485e-01],\n",
      "         [ 5.9517e-01],\n",
      "         [ 4.3731e-01],\n",
      "         [ 5.3635e-01],\n",
      "         [ 4.7069e-01],\n",
      "         [ 4.1696e-01],\n",
      "         [ 6.4639e-01],\n",
      "         [ 4.2241e-01],\n",
      "         [ 3.5925e-01],\n",
      "         [ 3.2833e-01],\n",
      "         [ 3.7496e-01],\n",
      "         [ 3.5311e-01],\n",
      "         [ 3.7080e-01]],\n",
      "\n",
      "        [[ 8.1766e-02],\n",
      "         [-9.9239e-02],\n",
      "         [-1.4638e-01],\n",
      "         [-1.2569e-01],\n",
      "         [-3.8941e-02],\n",
      "         [-2.0013e-01],\n",
      "         [-1.3408e-01],\n",
      "         [-5.5633e-02],\n",
      "         [-7.6402e-02],\n",
      "         [-1.7483e-02],\n",
      "         [-7.9198e-02],\n",
      "         [-5.4625e-02],\n",
      "         [ 1.6237e-01],\n",
      "         [-1.0378e-01],\n",
      "         [-1.0815e-01],\n",
      "         [-9.4098e-02],\n",
      "         [-1.6403e-01],\n",
      "         [-1.1638e-01],\n",
      "         [-1.1846e-01],\n",
      "         [-1.6589e-01],\n",
      "         [-1.4727e-01],\n",
      "         [-1.1786e-01],\n",
      "         [-1.2803e-01],\n",
      "         [-9.5555e-02]],\n",
      "\n",
      "        [[ 6.6414e-01],\n",
      "         [ 6.5246e-01],\n",
      "         [ 6.4179e-01],\n",
      "         [ 5.2887e-01],\n",
      "         [ 6.3609e-01],\n",
      "         [ 6.3148e-01],\n",
      "         [ 5.5823e-01],\n",
      "         [ 6.4409e-01],\n",
      "         [ 8.3820e-02],\n",
      "         [ 5.5487e-01],\n",
      "         [ 5.5256e-01],\n",
      "         [ 5.9509e-01],\n",
      "         [ 5.1239e-01],\n",
      "         [ 5.3664e-01],\n",
      "         [ 5.0079e-01],\n",
      "         [ 5.6930e-01],\n",
      "         [ 5.5542e-01],\n",
      "         [ 5.2900e-01],\n",
      "         [ 5.4712e-01],\n",
      "         [ 5.9712e-01],\n",
      "         [ 5.5663e-01],\n",
      "         [ 6.3237e-01],\n",
      "         [ 6.4783e-01],\n",
      "         [ 5.5444e-01]]], device='cuda:0', grad_fn=<AddBackward0>), ([], [], []))\n",
      ":List trace inputs must have elements (toTypeInferredIValue at /opt/conda/conda-bld/pytorch_1579027003190/work/torch/csrc/jit/pybind_utils.h:293)\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7fdd0ee39627 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x6e4c9f (0x7fdd40197c9f in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #2: <unknown function> + 0x769f4b (0x7fdd4021cf4b in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #3: torch::jit::tracer::trace(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::function<std::vector<c10::IValue, std::allocator<c10::IValue> > (std::vector<c10::IValue, std::allocator<c10::IValue> >)> const&, std::function<std::string (at::Tensor const&)>, bool, torch::jit::script::Module*) + 0x4e6 (0x7fdd14a3ee26 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch.so)\n",
      "frame #4: <unknown function> + 0x7660e1 (0x7fdd402190e1 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #5: <unknown function> + 0x77ffb1 (0x7fdd40232fb1 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #6: <unknown function> + 0x28c076 (0x7fdd3fd3f076 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\n",
      "frame #7: _PyCFunction_FastCallDict + 0x154 (0x5633d295f304 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #8: <unknown function> + 0x199c5e (0x5633d29e6c5e in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #9: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #10: <unknown function> + 0x19335e (0x5633d29e035e in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #11: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #12: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #13: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #14: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #15: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #16: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #17: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #18: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #19: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #20: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #21: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #22: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #23: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #24: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #25: _PyEval_EvalFrameDefault + 0x10c9 (0x5633d2a0a5d9 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #26: PyEval_EvalCodeEx + 0x329 (0x5633d29e1a49 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #27: PyEval_EvalCode + 0x1c (0x5633d29e27ec in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #28: <unknown function> + 0x1ba227 (0x5633d2a07227 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #29: _PyCFunction_FastCallDict + 0x91 (0x5633d295f241 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #30: <unknown function> + 0x199b0c (0x5633d29e6b0c in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #31: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #32: _PyGen_Send + 0x256 (0x5633d29e9bc6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #33: _PyEval_EvalFrameDefault + 0x1445 (0x5633d2a0a955 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #34: _PyGen_Send + 0x256 (0x5633d29e9bc6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #35: _PyEval_EvalFrameDefault + 0x1445 (0x5633d2a0a955 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #36: _PyGen_Send + 0x256 (0x5633d29e9bc6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #37: _PyCFunction_FastCallDict + 0x115 (0x5633d295f2c5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #38: <unknown function> + 0x199b0c (0x5633d29e6b0c in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #39: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #40: <unknown function> + 0x193cfb (0x5633d29e0cfb in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #41: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #42: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #43: <unknown function> + 0x193cfb (0x5633d29e0cfb in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #44: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #45: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #46: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #47: _PyFunction_FastCallDict + 0x3d8 (0x5633d29e1628 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #48: _PyObject_FastCallDict + 0x26f (0x5633d295f6cf in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #49: _PyObject_Call_Prepend + 0x63 (0x5633d2964143 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #50: PyObject_Call + 0x3e (0x5633d295f10e in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #51: _PyEval_EvalFrameDefault + 0x1aaf (0x5633d2a0afbf in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #52: <unknown function> + 0x1931f6 (0x5633d29e01f6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #53: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #54: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #55: _PyEval_EvalFrameDefault + 0x10c9 (0x5633d2a0a5d9 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #56: <unknown function> + 0x19c744 (0x5633d29e9744 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #57: _PyCFunction_FastCallDict + 0x91 (0x5633d295f241 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #58: <unknown function> + 0x199b0c (0x5633d29e6b0c in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #59: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #60: <unknown function> + 0x1931f6 (0x5633d29e01f6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #61: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #62: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "frame #63: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
      "\n",
      "Error occurs, No graph saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tracer cannot infer type of (tensor([[-23.9768, -17.3129, -18.0523,  ..., -18.6565, -18.3060, -14.8244],\n        [-26.2138, -18.8952, -17.7499,  ..., -17.4112, -15.3365, -17.8206],\n        [-24.8113, -16.7020, -11.6527,  ..., -15.2050, -14.3076, -10.0045],\n        ...,\n        [-27.0051, -15.3371, -20.5083,  ..., -19.9755, -16.9174, -17.5259],\n        [-21.7966, -14.3504, -19.0064,  ..., -13.6335, -11.8910, -26.7599],\n        [-23.3009, -11.6584, -19.6952,  ..., -18.6055, -13.8711, -18.0167]],\n       device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-11.6474, -10.8408,  -8.1201,  ..., -13.7666, -13.3638, -14.8178],\n        [-14.1130,  -8.2416, -11.8985,  ..., -15.3526, -14.5676, -16.1072],\n        [-10.0281,  -8.5481, -12.1849,  ...,  -9.9922, -13.6915, -11.6269],\n        ...,\n        [-20.1335, -18.3920, -17.5234,  ..., -20.9741, -23.5247, -22.5615],\n        [ -6.5798,  -8.0710,  -7.1975,  ...,  -9.0155,  -8.8422, -10.2870],\n        [-18.6119, -14.4927, -17.6004,  ..., -18.2182, -18.8035, -19.1235]],\n       device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-0.6393],\n        [-0.1429],\n        [-2.1546],\n        [-5.1147],\n        [-1.6715],\n        [ 0.3507],\n        [-4.3397],\n        [ 2.2866],\n        [-1.4691],\n        [ 1.4585],\n        [-0.2973],\n        [ 0.7474],\n        [ 2.5556],\n        [-2.9994],\n        [-0.9246],\n        [-1.2214],\n        [ 0.3344],\n        [ 2.2141],\n        [-0.7545],\n        [ 2.4200],\n        [-0.0531],\n        [ 3.1947],\n        [-0.2689],\n        [ 0.4529],\n        [-0.3984],\n        [-0.2024],\n        [ 1.1264],\n        [ 1.3671],\n        [-3.4349],\n        [-0.3805]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[ -1.8382,   2.1890],\n        [  9.7137,  -9.1303],\n        [  1.1245,  -0.7504],\n        [ -8.7560,   9.4857],\n        [-12.7439,  12.7932],\n        [-12.3451,  12.5709],\n        [ -1.1877,   0.6844],\n        [ -4.0932,   4.0128],\n        [ -4.8093,   5.3705],\n        [ -3.7177,   3.9173],\n        [-12.6056,  12.4244],\n        [ 10.6988, -10.2227],\n        [  2.8526,  -2.8399],\n        [ -7.0294,   7.1356],\n        [-10.3483,   9.9678]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-1.3022, -1.1211, -3.1991],\n        [-0.0492, -0.8463, -3.6581],\n        [-0.1244, -1.9052, -2.7197],\n        [-1.2288,  0.7291, -5.5015],\n        [-0.3299, -2.6304, -0.9506],\n        [-1.6865, -1.4229, -2.1603],\n        [ 0.4084, -1.3018, -4.3268],\n        [-3.0534, -1.7261,  0.4608],\n        [ 0.6957, -1.7683, -3.7300],\n        [-2.5832, -0.6753, -0.2382],\n        [-0.4761, -1.3521, -2.6981],\n        [-2.4724, -0.0574, -2.8649],\n        [-3.2873,  0.2401, -0.6161],\n        [ 2.8250, -3.0116, -5.6009],\n        [ 0.4068,  0.0392, -5.6889],\n        [ 0.2833, -1.2190, -3.7594],\n        [-1.5270, -1.1284, -2.1290],\n        [-2.2854, -2.1912,  0.3826],\n        [-0.8855, -0.2406, -2.5146],\n        [-2.2929, -1.9686,  0.3879],\n        [-0.9590, -0.1366, -3.1811],\n        [-3.6603, -1.3946,  1.0387],\n        [-2.8627,  0.4189, -2.4454],\n        [-0.9705, -0.0349, -1.1119],\n        [-1.2466, -2.2507, -3.0363],\n        [-0.8483, -2.6603, -3.1504],\n        [-2.7211, -2.8056, -0.4124],\n        [-0.7093, -1.7536, -2.3936],\n        [-1.6274,  0.5352, -3.8358],\n        [-2.8041, -0.9302, -1.3755]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[[  8.1071,  -5.0885,  -3.7664,  ...,  -1.4273,  -0.9021,  -4.1969],\n         [  3.8177,  -4.5532,  -2.8071,  ...,  -1.3409,  -1.7692,  -2.2690],\n         [  7.0173,  -5.5335,  -4.4290,  ...,  -1.3300,   0.1980,  -3.6034],\n         ...,\n         [  7.1785,  -6.1633,  -3.6649,  ...,  -1.6600,  -1.7262,  -5.0354],\n         [  6.9941,  -5.3046,  -2.6913,  ...,  -3.1742,  -2.2862,  -3.5778],\n         [  7.5086,  -5.9734,  -2.9408,  ...,  -3.7117,  -2.2481,  -3.2648]],\n\n        [[  7.4254,  -6.7579,  -4.9328,  ...,  -3.9803,  -2.2647,  -2.4394],\n         [  7.2897,  -5.9018,  -5.5442,  ...,  -1.9450,  -1.7594,  -4.7357],\n         [  5.6920,  -5.7744,  -4.4779,  ...,  -3.6136,  -1.5126,  -1.5384],\n         ...,\n         [  6.6963,  -6.7992,  -6.3011,  ...,  -3.5846,  -2.3700,  -3.7784],\n         [  7.2653,  -5.9123,  -5.5892,  ...,  -1.9841,  -1.7899,  -4.7843],\n         [  7.7138,  -5.0347,  -4.6088,  ...,  -3.1614,  -3.4179,  -4.0131]],\n\n        [[  5.2152,  -8.8324,  -3.6273,  ...,  -6.6085,  -4.7241,  -6.8234],\n         [  4.8676,  -4.3746,  -2.4917,  ...,  -3.3825,  -0.8355,  -2.7390],\n         [  4.8747,  -7.0232,  -4.1367,  ...,  -4.1859,  -2.7916,  -3.0367],\n         ...,\n         [  5.6844, -10.0907,  -4.7995,  ...,  -5.5661,  -3.4763,  -6.9165],\n         [  3.8252,  -4.4261,  -1.5736,  ...,  -3.6205,  -0.7457,  -2.6781],\n         [  5.5476,  -8.0797,  -5.3136,  ...,  -5.9175,  -4.6428,  -6.6182]],\n\n        ...,\n\n        [[  6.2329,  -5.6986,  -4.4003,  ...,  -4.9978,  -5.4505,  -6.0065],\n         [  4.5816,  -3.0084,  -4.3789,  ...,  -3.6890,  -0.4907,  -4.3853],\n         [  4.2864,  -4.2727,  -3.4970,  ...,  -3.9741,  -3.0780,  -6.3296],\n         ...,\n         [  6.5369,  -4.8309,  -2.7178,  ...,  -5.2795,  -4.7246,  -5.6874],\n         [  7.1938,  -2.4227,  -3.5248,  ...,  -4.8716,  -4.8112,  -6.5379],\n         [  6.7999,  -4.8870,  -0.2368,  ...,  -5.6579,  -5.1136,  -3.9159]],\n\n        [[  4.2967,  -6.3247,  -5.5420,  ...,  -4.6959,  -4.1590,  -5.7608],\n         [  5.8049,  -3.6484,  -4.5866,  ...,  -3.8342,  -1.8794,  -5.5023],\n         [  5.3276,  -3.8784,  -3.4268,  ...,  -3.0494,  -3.3516,  -5.3999],\n         ...,\n         [  4.5262,  -5.7328,  -4.5576,  ...,  -5.2571,  -4.0224,  -4.7971],\n         [  6.1058,  -2.9771,  -4.3419,  ...,  -3.4850,  -3.5584,  -6.0711],\n         [  6.0065,  -6.8486,  -2.9187,  ...,  -5.1517,  -3.8328,  -5.4005]],\n\n        [[  6.7952,  -5.9581,  -3.8170,  ...,  -6.1306,  -3.8980,  -5.2182],\n         [  4.7843,  -4.3625,  -3.4503,  ...,  -3.0429,  -1.0069,  -5.0160],\n         [  4.7423,  -4.0734,  -2.0026,  ...,  -3.3397,  -1.5319,  -5.5408],\n         ...,\n         [  5.9233,  -6.7720,  -3.2585,  ...,  -5.0303,  -3.6403,  -6.4752],\n         [  6.4932,  -3.8687,  -2.0943,  ...,  -4.3999,  -3.3642,  -6.5363],\n         [  7.4559,  -5.1406,   0.5284,  ...,  -4.9604,  -4.5710,  -5.3826]]],\n       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-1.0254],\n         [ 0.5791],\n         [-0.6458],\n         ...,\n         [-1.2198],\n         [-2.7965],\n         [-1.1531]],\n\n        [[-1.1604],\n         [-0.9972],\n         [-2.1179],\n         ...,\n         [-0.9099],\n         [-1.0286],\n         [-3.0328]],\n\n        [[-5.0549],\n         [-3.8171],\n         [-1.7807],\n         ...,\n         [-5.8512],\n         [-3.3489],\n         [-4.7341]],\n\n        ...,\n\n        [[-1.6947],\n         [-0.4835],\n         [-4.9195],\n         ...,\n         [-3.5398],\n         [-3.4552],\n         [-3.4297]],\n\n        [[ 0.4876],\n         [-2.0698],\n         [-3.0662],\n         ...,\n         [-1.2132],\n         [-1.7824],\n         [-0.7635]],\n\n        [[ 0.7829],\n         [ 0.2189],\n         [-2.1548],\n         ...,\n         [-1.7861],\n         [-2.7979],\n         [-1.9784]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-3.6498, -3.4908, -3.6187,  ..., -3.5289, -3.8178, -3.6744],\n         [-3.2804, -3.1006, -3.1543,  ..., -3.0870, -3.3823, -3.2175],\n         [-3.5100, -3.3237, -3.3740,  ..., -3.3072, -3.5707, -3.4169],\n         ...,\n         [-4.1081, -4.0421, -4.0378,  ..., -3.9809, -4.1430, -4.1447],\n         [-4.1650, -4.0987, -4.1115,  ..., -4.0453, -4.2243, -4.2235],\n         [-4.1392, -4.0736, -4.0614,  ..., -3.9937, -4.1944, -4.1707]],\n\n        [[-2.3112, -2.3721, -2.2133,  ..., -2.6004, -2.6427, -2.4511],\n         [-3.4823, -3.5827, -3.4849,  ..., -3.5824, -3.7505, -3.5713],\n         [-3.5046, -3.6621, -3.5683,  ..., -3.6990, -3.8633, -3.6441],\n         ...,\n         [-3.8630, -3.8739, -3.7105,  ..., -4.0762, -4.0627, -3.8866],\n         [-3.8149, -3.8647, -3.6870,  ..., -4.0637, -4.0323, -3.8605],\n         [-4.1886, -4.2119, -4.0773,  ..., -4.4352, -4.3988, -4.2430]],\n\n        [[-1.9862, -1.7931, -1.7450,  ..., -1.8751, -2.1345, -2.0390],\n         [-1.5100, -1.3702, -1.2807,  ..., -1.4770, -1.6623, -1.6035],\n         [-1.5638, -1.4044, -1.3180,  ..., -1.5494, -1.7191, -1.6515],\n         ...,\n         [-1.8217, -1.6298, -1.5178,  ..., -1.8839, -1.9941, -1.8687],\n         [-2.0018, -1.8120, -1.7015,  ..., -2.0700, -2.1824, -2.0502],\n         [-1.9469, -1.7752, -1.6727,  ..., -2.0368, -2.1384, -1.9946]],\n\n        ...,\n\n        [[-0.7459, -0.2746, -0.5147,  ..., -0.6378, -0.9469, -0.7759],\n         [-0.9958, -0.5589, -0.7966,  ..., -0.9018, -1.2208, -1.0370],\n         [-1.0953, -0.6611, -0.8872,  ..., -0.9099, -1.3346, -1.0890],\n         ...,\n         [-1.7204, -1.1448, -1.5489,  ..., -1.9175, -1.9201, -1.9237],\n         [-1.1636, -0.6134, -0.9931,  ..., -1.3438, -1.3703, -1.3533],\n         [-1.0375, -0.4929, -0.8435,  ..., -1.2396, -1.2365, -1.2116]],\n\n        [[-2.3704, -2.1079, -2.2421,  ..., -2.4508, -2.8402, -2.6562],\n         [-3.5595, -3.2445, -3.5233,  ..., -3.5955, -3.9896, -3.7924],\n         [-4.2090, -3.8361, -4.1596,  ..., -4.1861, -4.6080, -4.4536],\n         ...,\n         [-2.9149, -2.6139, -2.8405,  ..., -3.0460, -3.2139, -3.1900],\n         [-2.9831, -2.6284, -2.8564,  ..., -3.0792, -3.2402, -3.2508],\n         [-2.7952, -2.4774, -2.7095,  ..., -2.9133, -3.0668, -3.0817]],\n\n        [[-2.1046, -1.7263, -1.9545,  ..., -2.2074, -2.3714, -2.0817],\n         [-2.8306, -2.4761, -2.6339,  ..., -2.8985, -3.0506, -2.7840],\n         [-2.7562, -2.3964, -2.5503,  ..., -2.8107, -2.9543, -2.6950],\n         ...,\n         [-2.1971, -1.8356, -2.0023,  ..., -2.3088, -2.3486, -2.1684],\n         [-2.1272, -1.7860, -1.9482,  ..., -2.2151, -2.2926, -2.0856],\n         [-1.8418, -1.4547, -1.6640,  ..., -1.9770, -1.9930, -1.8990]]],\n       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[ 5.4066e-02],\n         [ 6.5631e-02],\n         [ 7.8555e-02],\n         [ 8.2301e-02],\n         [ 5.5858e-02],\n         [ 1.7058e-02],\n         [ 5.6861e-02],\n         [ 4.3544e-02],\n         [ 5.9106e-02],\n         [ 1.0269e-01],\n         [ 6.7681e-02],\n         [ 5.7432e-02],\n         [ 4.9514e-02],\n         [ 4.2039e-02],\n         [ 4.9348e-02],\n         [ 9.6293e-02],\n         [ 2.2876e-02],\n         [ 5.0893e-02],\n         [ 5.4959e-02],\n         [ 3.7836e-02],\n         [ 5.6841e-02],\n         [ 5.0191e-02],\n         [ 6.1113e-02],\n         [ 9.0875e-02]],\n\n        [[-6.7664e-02],\n         [-1.6427e-01],\n         [-1.6623e-01],\n         [-1.2173e-01],\n         [-1.8003e-01],\n         [-1.6219e-01],\n         [-1.0723e-02],\n         [-1.5421e-01],\n         [-4.9339e-02],\n         [-3.8281e-02],\n         [-1.8616e-01],\n         [ 2.1373e-01],\n         [-8.6579e-02],\n         [-9.1772e-02],\n         [-1.0777e-01],\n         [-1.2135e-01],\n         [-8.8904e-02],\n         [-1.5278e-01],\n         [-1.2438e-01],\n         [-1.0036e-01],\n         [-1.1380e-01],\n         [-9.2139e-02],\n         [-8.9316e-02],\n         [-1.1768e-01]],\n\n        [[ 6.5625e-02],\n         [ 6.0019e-02],\n         [ 5.6963e-02],\n         [ 4.7871e-02],\n         [ 1.7319e-02],\n         [ 4.6699e-02],\n         [-1.6525e-01],\n         [ 6.3119e-02],\n         [-1.7403e-02],\n         [ 8.5018e-02],\n         [ 1.1315e-01],\n         [ 1.1513e-01],\n         [ 1.0047e-01],\n         [ 9.8939e-02],\n         [ 7.9134e-02],\n         [ 1.0424e-01],\n         [ 9.2704e-02],\n         [ 1.2133e-01],\n         [ 9.9248e-02],\n         [ 4.4340e-02],\n         [ 8.7588e-02],\n         [ 1.3086e-01],\n         [ 1.1863e-01],\n         [ 9.6424e-02]],\n\n        [[ 5.2794e-02],\n         [ 1.0923e-01],\n         [ 1.1409e-01],\n         [ 7.6880e-02],\n         [ 5.6569e-02],\n         [ 6.6162e-02],\n         [ 1.1882e-01],\n         [-7.2202e-02],\n         [ 9.9407e-02],\n         [ 1.9248e-01],\n         [ 1.1154e-01],\n         [ 1.1839e-01],\n         [ 1.3504e-01],\n         [ 1.4178e-01],\n         [ 1.5760e-01],\n         [ 1.1420e-01],\n         [ 1.0585e-01],\n         [ 1.4212e-01],\n         [ 1.2363e-01],\n         [ 7.6701e-02],\n         [ 1.2803e-01],\n         [ 1.4405e-01],\n         [ 1.5990e-01],\n         [ 1.7584e-01]],\n\n        [[-1.7807e-01],\n         [-2.6471e-01],\n         [-2.6448e-01],\n         [-2.6246e-01],\n         [-1.7450e-01],\n         [-1.4318e-01],\n         [-2.0536e-01],\n         [-2.7054e-01],\n         [-2.6496e-01],\n         [ 2.3996e-02],\n         [-1.9643e-01],\n         [-1.8517e-01],\n         [-1.5990e-01],\n         [-1.4904e-01],\n         [-1.7138e-01],\n         [-1.4573e-01],\n         [-1.5216e-01],\n         [-1.3528e-01],\n         [-1.4958e-01],\n         [-1.6462e-01],\n         [-1.6598e-01],\n         [-2.1219e-01],\n         [-2.1703e-01],\n         [-1.9016e-01]],\n\n        [[-8.1026e-02],\n         [ 1.5849e-02],\n         [ 4.3204e-02],\n         [ 4.3463e-02],\n         [-9.3584e-03],\n         [ 5.5187e-03],\n         [ 3.1684e-02],\n         [ 3.5463e-02],\n         [ 3.6369e-02],\n         [ 1.6242e-02],\n         [ 2.1057e-01],\n         [ 6.2618e-02],\n         [ 4.8180e-02],\n         [ 1.1223e-01],\n         [ 8.8505e-02],\n         [ 9.5567e-02],\n         [ 6.4552e-02],\n         [ 3.1516e-02],\n         [ 1.1724e-01],\n         [ 7.3821e-02],\n         [ 4.7713e-02],\n         [ 6.2399e-02],\n         [ 7.8784e-02],\n         [ 7.6747e-02]],\n\n        [[-1.1578e-01],\n         [-1.4448e-01],\n         [-1.2108e-01],\n         [-1.1716e-01],\n         [-1.3413e-01],\n         [-6.0049e-02],\n         [-1.0755e-01],\n         [-1.5272e-01],\n         [-1.4546e-01],\n         [-2.6533e-01],\n         [-1.5269e-01],\n         [ 1.7279e-01],\n         [-1.9391e-01],\n         [-8.6409e-02],\n         [-1.7555e-01],\n         [-1.7146e-01],\n         [-1.5573e-01],\n         [-1.4953e-01],\n         [-2.7697e-02],\n         [-1.7865e-01],\n         [-1.1629e-01],\n         [-6.9994e-02],\n         [-9.2495e-02],\n         [-1.9928e-01]],\n\n        [[ 3.9459e-02],\n         [-5.4832e-02],\n         [-6.4811e-02],\n         [-8.4612e-02],\n         [-2.3616e-02],\n         [-5.4199e-02],\n         [-7.1516e-02],\n         [-4.1795e-02],\n         [-6.5328e-02],\n         [ 2.9066e-01],\n         [ 1.4585e-01],\n         [ 1.6468e-01],\n         [ 1.3082e-01],\n         [ 1.3881e-01],\n         [ 1.1905e-01],\n         [ 1.1631e-01],\n         [ 1.1235e-01],\n         [ 9.4683e-02],\n         [ 1.6876e-01],\n         [ 1.5931e-01],\n         [ 1.6417e-01],\n         [ 1.6234e-01],\n         [ 1.1294e-01],\n         [ 1.8581e-01]],\n\n        [[-6.7158e-02],\n         [-6.1223e-02],\n         [-8.2407e-02],\n         [-6.6869e-02],\n         [-5.5211e-02],\n         [ 1.2791e-02],\n         [-6.7059e-02],\n         [ 1.9043e-01],\n         [ 1.2470e-01],\n         [ 1.0622e-01],\n         [ 1.1262e-01],\n         [ 1.1784e-01],\n         [ 1.2425e-01],\n         [ 1.4761e-01],\n         [ 1.1366e-01],\n         [ 1.2769e-01],\n         [ 1.8647e-01],\n         [ 1.6956e-01],\n         [ 1.0577e-01],\n         [ 1.8000e-01],\n         [ 1.7869e-01],\n         [ 1.5395e-01],\n         [ 1.2786e-01],\n         [ 1.0609e-01]],\n\n        [[-8.6342e-02],\n         [-1.2166e-01],\n         [-2.7074e-01],\n         [-1.4543e-01],\n         [-1.5476e-01],\n         [-1.1159e-01],\n         [-8.4526e-02],\n         [-2.3460e-02],\n         [-5.5933e-02],\n         [-3.3472e-02],\n         [-1.7022e-03],\n         [-6.1379e-02],\n         [-4.8252e-02],\n         [-3.3536e-02],\n         [-4.7247e-02],\n         [ 6.0844e-02],\n         [ 3.9172e-02],\n         [-1.0941e-01],\n         [ 3.3746e-02],\n         [ 4.2120e-02],\n         [ 7.4103e-02],\n         [ 6.7480e-02],\n         [-3.9778e-02],\n         [ 4.6653e-02]],\n\n        [[ 1.1761e-04],\n         [-3.2319e-02],\n         [-8.2931e-02],\n         [-6.2712e-02],\n         [-5.5349e-02],\n         [-1.6789e-01],\n         [-5.7528e-02],\n         [-3.3631e-02],\n         [-8.9793e-02],\n         [-4.4528e-02],\n         [ 1.9845e-01],\n         [ 4.1326e-02],\n         [ 6.2142e-02],\n         [ 5.2120e-02],\n         [ 4.5284e-02],\n         [ 5.3875e-03],\n         [ 2.3621e-02],\n         [ 4.3163e-02],\n         [ 2.7918e-02],\n         [-2.7312e-03],\n         [ 2.5573e-02],\n         [ 2.2228e-02],\n         [ 2.3738e-02],\n         [ 4.5626e-02]],\n\n        [[ 1.1152e-01],\n         [ 1.1857e-01],\n         [ 1.2849e-01],\n         [ 1.0917e-01],\n         [ 9.5948e-02],\n         [ 5.3384e-02],\n         [ 1.0204e-03],\n         [ 1.0989e-01],\n         [ 6.0001e-02],\n         [ 1.5550e-01],\n         [ 1.5263e-01],\n         [ 1.9080e-01],\n         [ 1.5649e-01],\n         [ 1.7876e-01],\n         [ 1.4794e-01],\n         [ 1.8362e-01],\n         [ 3.1785e-01],\n         [ 1.4816e-01],\n         [ 1.9398e-01],\n         [ 2.9620e-01],\n         [ 1.6768e-01],\n         [ 2.6444e-01],\n         [ 1.7876e-01],\n         [ 1.6529e-01]],\n\n        [[-3.2135e-01],\n         [-2.7609e-01],\n         [-2.2708e-01],\n         [-2.7871e-01],\n         [-1.5976e-01],\n         [-1.0789e-01],\n         [-2.6115e-01],\n         [ 2.4042e-01],\n         [ 1.7649e-01],\n         [ 1.0470e-01],\n         [ 7.6834e-02],\n         [ 1.1821e-01],\n         [ 9.2225e-02],\n         [ 1.8485e-01],\n         [ 1.6472e-01],\n         [ 1.1662e-01],\n         [ 1.3644e-01],\n         [ 1.0456e-01],\n         [ 1.4690e-01],\n         [ 1.0390e-01],\n         [ 1.6453e-01],\n         [ 1.6165e-01],\n         [ 1.5546e-01],\n         [ 1.7893e-01]],\n\n        [[ 2.2485e-01],\n         [ 1.5733e-01],\n         [ 1.8498e-01],\n         [ 1.1078e-01],\n         [ 2.0727e-01],\n         [ 1.8171e-01],\n         [ 4.9812e-02],\n         [ 1.6734e-02],\n         [ 1.8851e-01],\n         [ 2.8564e-01],\n         [ 2.0382e-01],\n         [ 1.7996e-01],\n         [ 1.8341e-01],\n         [ 1.6929e-01],\n         [ 1.5521e-01],\n         [ 1.3484e-01],\n         [ 1.5028e-01],\n         [ 1.4007e-01],\n         [ 9.8987e-02],\n         [ 1.9394e-01],\n         [ 2.3100e-01],\n         [ 2.1777e-01],\n         [ 1.4207e-01],\n         [ 3.0390e-01]],\n\n        [[-2.8275e-01],\n         [-1.7993e-01],\n         [-1.3902e-01],\n         [-1.6490e-01],\n         [-1.0496e-01],\n         [-2.0422e-01],\n         [-1.8531e-01],\n         [-1.7398e-01],\n         [-2.2590e-01],\n         [-1.9143e-01],\n         [ 3.7845e-01],\n         [-1.4968e-01],\n         [-1.7335e-01],\n         [-2.0042e-01],\n         [-1.4693e-01],\n         [-1.1373e-01],\n         [-4.5550e-02],\n         [-1.5862e-01],\n         [-1.9468e-01],\n         [-9.1910e-02],\n         [-2.0067e-01],\n         [-1.8242e-01],\n         [-2.3379e-01],\n         [-2.4639e-01]],\n\n        [[ 3.5746e-02],\n         [ 1.1198e-01],\n         [ 1.5230e-01],\n         [ 2.2968e-01],\n         [ 1.2334e-01],\n         [ 7.9760e-02],\n         [ 2.4281e-02],\n         [-1.0909e-02],\n         [ 1.1329e-01],\n         [ 2.1473e-01],\n         [ 1.6258e-01],\n         [ 1.7210e-01],\n         [ 1.3568e-01],\n         [ 9.7467e-02],\n         [ 8.7748e-02],\n         [ 1.7078e-01],\n         [ 1.8598e-01],\n         [ 8.9025e-02],\n         [ 1.1402e-01],\n         [ 1.6093e-01],\n         [ 2.7516e-01],\n         [ 1.6528e-01],\n         [ 1.5849e-01],\n         [ 1.4951e-01]],\n\n        [[-6.7726e-02],\n         [-2.6111e-02],\n         [-3.7656e-02],\n         [-3.9180e-02],\n         [-2.5971e-02],\n         [-1.4467e-01],\n         [-1.0697e-02],\n         [-1.7789e-02],\n         [-8.8801e-02],\n         [-1.5108e-01],\n         [-3.3329e-02],\n         [ 2.3654e-01],\n         [ 2.0878e-02],\n         [ 2.2508e-02],\n         [ 2.8622e-02],\n         [-1.1708e-02],\n         [-4.0135e-02],\n         [ 2.9418e-02],\n         [-9.4731e-03],\n         [-2.9228e-03],\n         [-2.7782e-02],\n         [ 2.2871e-02],\n         [ 1.7818e-02],\n         [ 2.9181e-02]],\n\n        [[ 4.7783e-02],\n         [ 8.7086e-02],\n         [ 7.9555e-02],\n         [ 6.4638e-02],\n         [ 1.0188e-02],\n         [-7.4722e-03],\n         [ 7.2756e-02],\n         [ 7.3916e-02],\n         [ 7.0337e-02],\n         [ 1.6638e-01],\n         [ 9.5175e-02],\n         [ 8.1817e-02],\n         [ 1.5183e-01],\n         [ 7.3419e-02],\n         [ 1.0479e-01],\n         [ 8.2887e-02],\n         [ 6.3357e-02],\n         [ 1.0268e-01],\n         [ 1.0171e-01],\n         [ 7.6174e-02],\n         [ 8.9681e-02],\n         [ 9.0263e-02],\n         [ 7.2253e-02],\n         [ 4.0903e-02]],\n\n        [[-1.8114e-02],\n         [-3.1521e-02],\n         [-3.0011e-02],\n         [-3.3700e-02],\n         [-3.9190e-02],\n         [-7.1181e-02],\n         [-1.3108e-01],\n         [-2.4210e-02],\n         [-3.9979e-02],\n         [-5.0791e-02],\n         [-7.0238e-03],\n         [-4.6672e-02],\n         [ 3.8189e-01],\n         [-3.8349e-02],\n         [-8.6204e-02],\n         [-3.9688e-02],\n         [ 1.9305e-02],\n         [-6.7339e-02],\n         [-3.3100e-02],\n         [ 5.0904e-03],\n         [-1.5047e-02],\n         [-4.9961e-04],\n         [-1.9295e-02],\n         [-1.3981e-02]],\n\n        [[ 1.4099e-02],\n         [ 6.8466e-02],\n         [ 7.5729e-02],\n         [ 6.5672e-02],\n         [ 5.7519e-02],\n         [ 1.3579e-03],\n         [ 9.5770e-04],\n         [ 1.6213e-02],\n         [ 1.4458e-02],\n         [ 9.2938e-04],\n         [ 7.3183e-04],\n         [ 2.2929e-02],\n         [ 1.6425e-01],\n         [ 1.3149e-01],\n         [ 1.3060e-01],\n         [ 1.2997e-01],\n         [ 1.4727e-01],\n         [ 1.5043e-01],\n         [ 1.4297e-01],\n         [ 1.5185e-01],\n         [ 1.2384e-01],\n         [ 1.8071e-01],\n         [ 9.4006e-02],\n         [ 1.1730e-01]],\n\n        [[-2.9610e-01],\n         [-2.3449e-01],\n         [-2.2987e-01],\n         [-2.7641e-01],\n         [-2.4688e-01],\n         [-3.2661e-01],\n         [-2.6682e-01],\n         [-2.5042e-01],\n         [-3.3929e-01],\n         [-4.5027e-01],\n         [-2.1687e-01],\n         [-2.9021e-01],\n         [-2.4233e-01],\n         [ 2.8537e-01],\n         [-2.8036e-01],\n         [-2.9515e-01],\n         [-3.0914e-01],\n         [-3.1012e-01],\n         [-3.1859e-01],\n         [-3.0750e-01],\n         [-3.0444e-01],\n         [-2.5483e-01],\n         [-2.9749e-01],\n         [-2.7640e-01]],\n\n        [[-5.2741e-01],\n         [-4.1537e-01],\n         [-3.7248e-01],\n         [-3.3458e-01],\n         [-2.7509e-01],\n         [-3.5539e-01],\n         [-4.7727e-01],\n         [-7.2280e-01],\n         [-6.3663e-01],\n         [-6.8911e-01],\n         [-5.5427e-01],\n         [-3.6727e-01],\n         [ 1.5880e-01],\n         [-5.3511e-01],\n         [-5.0268e-01],\n         [-5.0363e-01],\n         [-3.7419e-01],\n         [-3.0913e-01],\n         [-5.0107e-01],\n         [-3.5062e-01],\n         [-5.1421e-01],\n         [-5.7398e-01],\n         [-7.1038e-01],\n         [-5.1138e-01]],\n\n        [[-4.6731e-01],\n         [-4.7546e-01],\n         [-4.7340e-01],\n         [-5.3868e-01],\n         [-4.6113e-01],\n         [-4.7867e-01],\n         [-5.9746e-01],\n         [-5.4122e-01],\n         [-5.6321e-01],\n         [-4.7033e-01],\n         [-4.6819e-01],\n         [ 7.6297e-02],\n         [-4.5826e-01],\n         [-4.3651e-01],\n         [-4.4497e-01],\n         [-4.8697e-01],\n         [-4.5469e-01],\n         [-4.2812e-01],\n         [-4.5190e-01],\n         [-4.4646e-01],\n         [-4.7458e-01],\n         [-4.2964e-01],\n         [-4.4461e-01],\n         [-4.3520e-01]],\n\n        [[ 2.3901e-02],\n         [-8.3929e-02],\n         [-5.9767e-02],\n         [-7.8982e-02],\n         [-9.1446e-02],\n         [-7.5392e-02],\n         [-8.0575e-02],\n         [-1.6147e-01],\n         [-1.3367e-01],\n         [-1.0356e-01],\n         [ 2.7246e-01],\n         [-6.2741e-02],\n         [-8.1526e-02],\n         [-8.0032e-02],\n         [-1.0672e-01],\n         [-1.1291e-01],\n         [-1.1773e-01],\n         [-1.0829e-01],\n         [-9.8497e-02],\n         [-8.1571e-02],\n         [-8.9528e-02],\n         [-5.8892e-02],\n         [-6.5702e-02],\n         [-6.0051e-02]],\n\n        [[ 4.3231e-01],\n         [ 3.1132e-01],\n         [ 3.0673e-01],\n         [ 3.0883e-01],\n         [ 3.2686e-01],\n         [ 4.0427e-01],\n         [ 2.9928e-01],\n         [ 2.4772e-01],\n         [ 2.8525e-01],\n         [ 2.3264e-01],\n         [ 3.2084e-01],\n         [ 4.6697e-01],\n         [ 3.0005e-01],\n         [ 3.2966e-01],\n         [ 3.1251e-01],\n         [ 2.9748e-01],\n         [ 2.9741e-01],\n         [ 3.1698e-01],\n         [ 3.3682e-01],\n         [ 3.0614e-01],\n         [ 2.9851e-01],\n         [ 3.3857e-01],\n         [ 3.3640e-01],\n         [ 3.2906e-01]],\n\n        [[ 3.7814e-01],\n         [ 2.7629e-01],\n         [ 2.7702e-01],\n         [ 2.7938e-01],\n         [ 3.0291e-01],\n         [ 4.2052e-01],\n         [ 3.9039e-01],\n         [ 2.7374e-01],\n         [ 2.7679e-01],\n         [ 2.8592e-01],\n         [ 3.5162e-01],\n         [ 3.4790e-01],\n         [ 3.0891e-01],\n         [ 3.0513e-01],\n         [ 3.5150e-01],\n         [ 3.5958e-01],\n         [ 4.0797e-01],\n         [ 3.0780e-01],\n         [ 3.3473e-01],\n         [ 4.5103e-01],\n         [ 3.3080e-01],\n         [ 3.7032e-01],\n         [ 3.1657e-01],\n         [ 3.1476e-01]],\n\n        [[ 2.6813e-01],\n         [ 2.0210e-01],\n         [ 2.1139e-01],\n         [ 1.7551e-01],\n         [ 1.8097e-01],\n         [ 1.8400e-01],\n         [ 1.7217e-01],\n         [ 1.8994e-01],\n         [ 4.3222e-01],\n         [ 2.5479e-01],\n         [ 2.0056e-01],\n         [ 2.4933e-01],\n         [ 2.1955e-01],\n         [ 2.4208e-01],\n         [ 1.8911e-01],\n         [ 1.9604e-01],\n         [ 1.7348e-01],\n         [ 2.9323e-01],\n         [ 2.9433e-01],\n         [ 2.2136e-01],\n         [ 3.0027e-01],\n         [ 3.0404e-01],\n         [ 3.4177e-01],\n         [ 3.1198e-01]],\n\n        [[ 3.2656e-01],\n         [ 2.8771e-01],\n         [ 2.9052e-01],\n         [ 2.7728e-01],\n         [ 4.5873e-01],\n         [ 2.4484e-01],\n         [ 2.7377e-01],\n         [ 4.9087e-01],\n         [ 3.1983e-01],\n         [ 2.7421e-01],\n         [ 2.1840e-01],\n         [ 3.6485e-01],\n         [ 5.9517e-01],\n         [ 4.3731e-01],\n         [ 5.3635e-01],\n         [ 4.7069e-01],\n         [ 4.1696e-01],\n         [ 6.4639e-01],\n         [ 4.2241e-01],\n         [ 3.5925e-01],\n         [ 3.2833e-01],\n         [ 3.7496e-01],\n         [ 3.5311e-01],\n         [ 3.7080e-01]],\n\n        [[ 8.1766e-02],\n         [-9.9239e-02],\n         [-1.4638e-01],\n         [-1.2569e-01],\n         [-3.8941e-02],\n         [-2.0013e-01],\n         [-1.3408e-01],\n         [-5.5633e-02],\n         [-7.6402e-02],\n         [-1.7483e-02],\n         [-7.9198e-02],\n         [-5.4625e-02],\n         [ 1.6237e-01],\n         [-1.0378e-01],\n         [-1.0815e-01],\n         [-9.4098e-02],\n         [-1.6403e-01],\n         [-1.1638e-01],\n         [-1.1846e-01],\n         [-1.6589e-01],\n         [-1.4727e-01],\n         [-1.1786e-01],\n         [-1.2803e-01],\n         [-9.5555e-02]],\n\n        [[ 6.6414e-01],\n         [ 6.5246e-01],\n         [ 6.4179e-01],\n         [ 5.2887e-01],\n         [ 6.3609e-01],\n         [ 6.3148e-01],\n         [ 5.5823e-01],\n         [ 6.4409e-01],\n         [ 8.3820e-02],\n         [ 5.5487e-01],\n         [ 5.5256e-01],\n         [ 5.9509e-01],\n         [ 5.1239e-01],\n         [ 5.3664e-01],\n         [ 5.0079e-01],\n         [ 5.6930e-01],\n         [ 5.5542e-01],\n         [ 5.2900e-01],\n         [ 5.4712e-01],\n         [ 5.9712e-01],\n         [ 5.5663e-01],\n         [ 6.3237e-01],\n         [ 6.4783e-01],\n         [ 5.5444e-01]]], device='cuda:0', grad_fn=<AddBackward0>), ([], [], []))\n:List trace inputs must have elements (toTypeInferredIValue at /opt/conda/conda-bld/pytorch_1579027003190/work/torch/csrc/jit/pybind_utils.h:293)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7fdd0ee39627 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x6e4c9f (0x7fdd40197c9f in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #2: <unknown function> + 0x769f4b (0x7fdd4021cf4b in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #3: torch::jit::tracer::trace(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::function<std::vector<c10::IValue, std::allocator<c10::IValue> > (std::vector<c10::IValue, std::allocator<c10::IValue> >)> const&, std::function<std::string (at::Tensor const&)>, bool, torch::jit::script::Module*) + 0x4e6 (0x7fdd14a3ee26 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch.so)\nframe #4: <unknown function> + 0x7660e1 (0x7fdd402190e1 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #5: <unknown function> + 0x77ffb1 (0x7fdd40232fb1 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #6: <unknown function> + 0x28c076 (0x7fdd3fd3f076 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #7: _PyCFunction_FastCallDict + 0x154 (0x5633d295f304 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #8: <unknown function> + 0x199c5e (0x5633d29e6c5e in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #9: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #10: <unknown function> + 0x19335e (0x5633d29e035e in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #11: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #12: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #13: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #14: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #15: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #16: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #17: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #18: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #19: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #20: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #21: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #22: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #23: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #24: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #25: _PyEval_EvalFrameDefault + 0x10c9 (0x5633d2a0a5d9 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #26: PyEval_EvalCodeEx + 0x329 (0x5633d29e1a49 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #27: PyEval_EvalCode + 0x1c (0x5633d29e27ec in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #28: <unknown function> + 0x1ba227 (0x5633d2a07227 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #29: _PyCFunction_FastCallDict + 0x91 (0x5633d295f241 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #30: <unknown function> + 0x199b0c (0x5633d29e6b0c in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #31: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #32: _PyGen_Send + 0x256 (0x5633d29e9bc6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #33: _PyEval_EvalFrameDefault + 0x1445 (0x5633d2a0a955 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #34: _PyGen_Send + 0x256 (0x5633d29e9bc6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #35: _PyEval_EvalFrameDefault + 0x1445 (0x5633d2a0a955 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #36: _PyGen_Send + 0x256 (0x5633d29e9bc6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #37: _PyCFunction_FastCallDict + 0x115 (0x5633d295f2c5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #38: <unknown function> + 0x199b0c (0x5633d29e6b0c in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #39: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #40: <unknown function> + 0x193cfb (0x5633d29e0cfb in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #41: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #42: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #43: <unknown function> + 0x193cfb (0x5633d29e0cfb in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #44: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #45: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #46: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #47: _PyFunction_FastCallDict + 0x3d8 (0x5633d29e1628 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #48: _PyObject_FastCallDict + 0x26f (0x5633d295f6cf in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #49: _PyObject_Call_Prepend + 0x63 (0x5633d2964143 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #50: PyObject_Call + 0x3e (0x5633d295f10e in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #51: _PyEval_EvalFrameDefault + 0x1aaf (0x5633d2a0afbf in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #52: <unknown function> + 0x1931f6 (0x5633d29e01f6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #53: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #54: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x10c9 (0x5633d2a0a5d9 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #56: <unknown function> + 0x19c744 (0x5633d29e9744 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #57: _PyCFunction_FastCallDict + 0x91 (0x5633d295f241 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #58: <unknown function> + 0x199b0c (0x5633d29e6b0c in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #59: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #60: <unknown function> + 0x1931f6 (0x5633d29e01f6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #61: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #62: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #63: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-dbc0b7c8906d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'results/runs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mco_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_to_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorboardX/writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, model, input_to_model, verbose)\u001b[0m\n\u001b[1;32m    792\u001b[0m         \"\"\"\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytorch_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_to_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_graph_deprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_to_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile_with_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/utils/tensorboard/_pytorch_graph.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error occurs, No graph saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/utils/tensorboard/_pytorch_graph.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: move outside of torch.onnx?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_inline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    880\u001b[0m         return trace_module(func, {'forward': example_inputs}, None,\n\u001b[1;32m    881\u001b[0m                             \u001b[0mcheck_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m                             check_tolerance, _force_outplace, _module_class)\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     if (hasattr(func, '__self__') and isinstance(func.__self__, torch.nn.Module) and\n",
      "\u001b[0;32m~/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmethod_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"forward\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0mexample_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_method_from_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_lookup_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tracer cannot infer type of (tensor([[-23.9768, -17.3129, -18.0523,  ..., -18.6565, -18.3060, -14.8244],\n        [-26.2138, -18.8952, -17.7499,  ..., -17.4112, -15.3365, -17.8206],\n        [-24.8113, -16.7020, -11.6527,  ..., -15.2050, -14.3076, -10.0045],\n        ...,\n        [-27.0051, -15.3371, -20.5083,  ..., -19.9755, -16.9174, -17.5259],\n        [-21.7966, -14.3504, -19.0064,  ..., -13.6335, -11.8910, -26.7599],\n        [-23.3009, -11.6584, -19.6952,  ..., -18.6055, -13.8711, -18.0167]],\n       device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-11.6474, -10.8408,  -8.1201,  ..., -13.7666, -13.3638, -14.8178],\n        [-14.1130,  -8.2416, -11.8985,  ..., -15.3526, -14.5676, -16.1072],\n        [-10.0281,  -8.5481, -12.1849,  ...,  -9.9922, -13.6915, -11.6269],\n        ...,\n        [-20.1335, -18.3920, -17.5234,  ..., -20.9741, -23.5247, -22.5615],\n        [ -6.5798,  -8.0710,  -7.1975,  ...,  -9.0155,  -8.8422, -10.2870],\n        [-18.6119, -14.4927, -17.6004,  ..., -18.2182, -18.8035, -19.1235]],\n       device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-0.6393],\n        [-0.1429],\n        [-2.1546],\n        [-5.1147],\n        [-1.6715],\n        [ 0.3507],\n        [-4.3397],\n        [ 2.2866],\n        [-1.4691],\n        [ 1.4585],\n        [-0.2973],\n        [ 0.7474],\n        [ 2.5556],\n        [-2.9994],\n        [-0.9246],\n        [-1.2214],\n        [ 0.3344],\n        [ 2.2141],\n        [-0.7545],\n        [ 2.4200],\n        [-0.0531],\n        [ 3.1947],\n        [-0.2689],\n        [ 0.4529],\n        [-0.3984],\n        [-0.2024],\n        [ 1.1264],\n        [ 1.3671],\n        [-3.4349],\n        [-0.3805]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[ -1.8382,   2.1890],\n        [  9.7137,  -9.1303],\n        [  1.1245,  -0.7504],\n        [ -8.7560,   9.4857],\n        [-12.7439,  12.7932],\n        [-12.3451,  12.5709],\n        [ -1.1877,   0.6844],\n        [ -4.0932,   4.0128],\n        [ -4.8093,   5.3705],\n        [ -3.7177,   3.9173],\n        [-12.6056,  12.4244],\n        [ 10.6988, -10.2227],\n        [  2.8526,  -2.8399],\n        [ -7.0294,   7.1356],\n        [-10.3483,   9.9678]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[-1.3022, -1.1211, -3.1991],\n        [-0.0492, -0.8463, -3.6581],\n        [-0.1244, -1.9052, -2.7197],\n        [-1.2288,  0.7291, -5.5015],\n        [-0.3299, -2.6304, -0.9506],\n        [-1.6865, -1.4229, -2.1603],\n        [ 0.4084, -1.3018, -4.3268],\n        [-3.0534, -1.7261,  0.4608],\n        [ 0.6957, -1.7683, -3.7300],\n        [-2.5832, -0.6753, -0.2382],\n        [-0.4761, -1.3521, -2.6981],\n        [-2.4724, -0.0574, -2.8649],\n        [-3.2873,  0.2401, -0.6161],\n        [ 2.8250, -3.0116, -5.6009],\n        [ 0.4068,  0.0392, -5.6889],\n        [ 0.2833, -1.2190, -3.7594],\n        [-1.5270, -1.1284, -2.1290],\n        [-2.2854, -2.1912,  0.3826],\n        [-0.8855, -0.2406, -2.5146],\n        [-2.2929, -1.9686,  0.3879],\n        [-0.9590, -0.1366, -3.1811],\n        [-3.6603, -1.3946,  1.0387],\n        [-2.8627,  0.4189, -2.4454],\n        [-0.9705, -0.0349, -1.1119],\n        [-1.2466, -2.2507, -3.0363],\n        [-0.8483, -2.6603, -3.1504],\n        [-2.7211, -2.8056, -0.4124],\n        [-0.7093, -1.7536, -2.3936],\n        [-1.6274,  0.5352, -3.8358],\n        [-2.8041, -0.9302, -1.3755]], device='cuda:0', grad_fn=<AddmmBackward>), tensor([[[  8.1071,  -5.0885,  -3.7664,  ...,  -1.4273,  -0.9021,  -4.1969],\n         [  3.8177,  -4.5532,  -2.8071,  ...,  -1.3409,  -1.7692,  -2.2690],\n         [  7.0173,  -5.5335,  -4.4290,  ...,  -1.3300,   0.1980,  -3.6034],\n         ...,\n         [  7.1785,  -6.1633,  -3.6649,  ...,  -1.6600,  -1.7262,  -5.0354],\n         [  6.9941,  -5.3046,  -2.6913,  ...,  -3.1742,  -2.2862,  -3.5778],\n         [  7.5086,  -5.9734,  -2.9408,  ...,  -3.7117,  -2.2481,  -3.2648]],\n\n        [[  7.4254,  -6.7579,  -4.9328,  ...,  -3.9803,  -2.2647,  -2.4394],\n         [  7.2897,  -5.9018,  -5.5442,  ...,  -1.9450,  -1.7594,  -4.7357],\n         [  5.6920,  -5.7744,  -4.4779,  ...,  -3.6136,  -1.5126,  -1.5384],\n         ...,\n         [  6.6963,  -6.7992,  -6.3011,  ...,  -3.5846,  -2.3700,  -3.7784],\n         [  7.2653,  -5.9123,  -5.5892,  ...,  -1.9841,  -1.7899,  -4.7843],\n         [  7.7138,  -5.0347,  -4.6088,  ...,  -3.1614,  -3.4179,  -4.0131]],\n\n        [[  5.2152,  -8.8324,  -3.6273,  ...,  -6.6085,  -4.7241,  -6.8234],\n         [  4.8676,  -4.3746,  -2.4917,  ...,  -3.3825,  -0.8355,  -2.7390],\n         [  4.8747,  -7.0232,  -4.1367,  ...,  -4.1859,  -2.7916,  -3.0367],\n         ...,\n         [  5.6844, -10.0907,  -4.7995,  ...,  -5.5661,  -3.4763,  -6.9165],\n         [  3.8252,  -4.4261,  -1.5736,  ...,  -3.6205,  -0.7457,  -2.6781],\n         [  5.5476,  -8.0797,  -5.3136,  ...,  -5.9175,  -4.6428,  -6.6182]],\n\n        ...,\n\n        [[  6.2329,  -5.6986,  -4.4003,  ...,  -4.9978,  -5.4505,  -6.0065],\n         [  4.5816,  -3.0084,  -4.3789,  ...,  -3.6890,  -0.4907,  -4.3853],\n         [  4.2864,  -4.2727,  -3.4970,  ...,  -3.9741,  -3.0780,  -6.3296],\n         ...,\n         [  6.5369,  -4.8309,  -2.7178,  ...,  -5.2795,  -4.7246,  -5.6874],\n         [  7.1938,  -2.4227,  -3.5248,  ...,  -4.8716,  -4.8112,  -6.5379],\n         [  6.7999,  -4.8870,  -0.2368,  ...,  -5.6579,  -5.1136,  -3.9159]],\n\n        [[  4.2967,  -6.3247,  -5.5420,  ...,  -4.6959,  -4.1590,  -5.7608],\n         [  5.8049,  -3.6484,  -4.5866,  ...,  -3.8342,  -1.8794,  -5.5023],\n         [  5.3276,  -3.8784,  -3.4268,  ...,  -3.0494,  -3.3516,  -5.3999],\n         ...,\n         [  4.5262,  -5.7328,  -4.5576,  ...,  -5.2571,  -4.0224,  -4.7971],\n         [  6.1058,  -2.9771,  -4.3419,  ...,  -3.4850,  -3.5584,  -6.0711],\n         [  6.0065,  -6.8486,  -2.9187,  ...,  -5.1517,  -3.8328,  -5.4005]],\n\n        [[  6.7952,  -5.9581,  -3.8170,  ...,  -6.1306,  -3.8980,  -5.2182],\n         [  4.7843,  -4.3625,  -3.4503,  ...,  -3.0429,  -1.0069,  -5.0160],\n         [  4.7423,  -4.0734,  -2.0026,  ...,  -3.3397,  -1.5319,  -5.5408],\n         ...,\n         [  5.9233,  -6.7720,  -3.2585,  ...,  -5.0303,  -3.6403,  -6.4752],\n         [  6.4932,  -3.8687,  -2.0943,  ...,  -4.3999,  -3.3642,  -6.5363],\n         [  7.4559,  -5.1406,   0.5284,  ...,  -4.9604,  -4.5710,  -5.3826]]],\n       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-1.0254],\n         [ 0.5791],\n         [-0.6458],\n         ...,\n         [-1.2198],\n         [-2.7965],\n         [-1.1531]],\n\n        [[-1.1604],\n         [-0.9972],\n         [-2.1179],\n         ...,\n         [-0.9099],\n         [-1.0286],\n         [-3.0328]],\n\n        [[-5.0549],\n         [-3.8171],\n         [-1.7807],\n         ...,\n         [-5.8512],\n         [-3.3489],\n         [-4.7341]],\n\n        ...,\n\n        [[-1.6947],\n         [-0.4835],\n         [-4.9195],\n         ...,\n         [-3.5398],\n         [-3.4552],\n         [-3.4297]],\n\n        [[ 0.4876],\n         [-2.0698],\n         [-3.0662],\n         ...,\n         [-1.2132],\n         [-1.7824],\n         [-0.7635]],\n\n        [[ 0.7829],\n         [ 0.2189],\n         [-2.1548],\n         ...,\n         [-1.7861],\n         [-2.7979],\n         [-1.9784]]], device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-3.6498, -3.4908, -3.6187,  ..., -3.5289, -3.8178, -3.6744],\n         [-3.2804, -3.1006, -3.1543,  ..., -3.0870, -3.3823, -3.2175],\n         [-3.5100, -3.3237, -3.3740,  ..., -3.3072, -3.5707, -3.4169],\n         ...,\n         [-4.1081, -4.0421, -4.0378,  ..., -3.9809, -4.1430, -4.1447],\n         [-4.1650, -4.0987, -4.1115,  ..., -4.0453, -4.2243, -4.2235],\n         [-4.1392, -4.0736, -4.0614,  ..., -3.9937, -4.1944, -4.1707]],\n\n        [[-2.3112, -2.3721, -2.2133,  ..., -2.6004, -2.6427, -2.4511],\n         [-3.4823, -3.5827, -3.4849,  ..., -3.5824, -3.7505, -3.5713],\n         [-3.5046, -3.6621, -3.5683,  ..., -3.6990, -3.8633, -3.6441],\n         ...,\n         [-3.8630, -3.8739, -3.7105,  ..., -4.0762, -4.0627, -3.8866],\n         [-3.8149, -3.8647, -3.6870,  ..., -4.0637, -4.0323, -3.8605],\n         [-4.1886, -4.2119, -4.0773,  ..., -4.4352, -4.3988, -4.2430]],\n\n        [[-1.9862, -1.7931, -1.7450,  ..., -1.8751, -2.1345, -2.0390],\n         [-1.5100, -1.3702, -1.2807,  ..., -1.4770, -1.6623, -1.6035],\n         [-1.5638, -1.4044, -1.3180,  ..., -1.5494, -1.7191, -1.6515],\n         ...,\n         [-1.8217, -1.6298, -1.5178,  ..., -1.8839, -1.9941, -1.8687],\n         [-2.0018, -1.8120, -1.7015,  ..., -2.0700, -2.1824, -2.0502],\n         [-1.9469, -1.7752, -1.6727,  ..., -2.0368, -2.1384, -1.9946]],\n\n        ...,\n\n        [[-0.7459, -0.2746, -0.5147,  ..., -0.6378, -0.9469, -0.7759],\n         [-0.9958, -0.5589, -0.7966,  ..., -0.9018, -1.2208, -1.0370],\n         [-1.0953, -0.6611, -0.8872,  ..., -0.9099, -1.3346, -1.0890],\n         ...,\n         [-1.7204, -1.1448, -1.5489,  ..., -1.9175, -1.9201, -1.9237],\n         [-1.1636, -0.6134, -0.9931,  ..., -1.3438, -1.3703, -1.3533],\n         [-1.0375, -0.4929, -0.8435,  ..., -1.2396, -1.2365, -1.2116]],\n\n        [[-2.3704, -2.1079, -2.2421,  ..., -2.4508, -2.8402, -2.6562],\n         [-3.5595, -3.2445, -3.5233,  ..., -3.5955, -3.9896, -3.7924],\n         [-4.2090, -3.8361, -4.1596,  ..., -4.1861, -4.6080, -4.4536],\n         ...,\n         [-2.9149, -2.6139, -2.8405,  ..., -3.0460, -3.2139, -3.1900],\n         [-2.9831, -2.6284, -2.8564,  ..., -3.0792, -3.2402, -3.2508],\n         [-2.7952, -2.4774, -2.7095,  ..., -2.9133, -3.0668, -3.0817]],\n\n        [[-2.1046, -1.7263, -1.9545,  ..., -2.2074, -2.3714, -2.0817],\n         [-2.8306, -2.4761, -2.6339,  ..., -2.8985, -3.0506, -2.7840],\n         [-2.7562, -2.3964, -2.5503,  ..., -2.8107, -2.9543, -2.6950],\n         ...,\n         [-2.1971, -1.8356, -2.0023,  ..., -2.3088, -2.3486, -2.1684],\n         [-2.1272, -1.7860, -1.9482,  ..., -2.2151, -2.2926, -2.0856],\n         [-1.8418, -1.4547, -1.6640,  ..., -1.9770, -1.9930, -1.8990]]],\n       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[ 5.4066e-02],\n         [ 6.5631e-02],\n         [ 7.8555e-02],\n         [ 8.2301e-02],\n         [ 5.5858e-02],\n         [ 1.7058e-02],\n         [ 5.6861e-02],\n         [ 4.3544e-02],\n         [ 5.9106e-02],\n         [ 1.0269e-01],\n         [ 6.7681e-02],\n         [ 5.7432e-02],\n         [ 4.9514e-02],\n         [ 4.2039e-02],\n         [ 4.9348e-02],\n         [ 9.6293e-02],\n         [ 2.2876e-02],\n         [ 5.0893e-02],\n         [ 5.4959e-02],\n         [ 3.7836e-02],\n         [ 5.6841e-02],\n         [ 5.0191e-02],\n         [ 6.1113e-02],\n         [ 9.0875e-02]],\n\n        [[-6.7664e-02],\n         [-1.6427e-01],\n         [-1.6623e-01],\n         [-1.2173e-01],\n         [-1.8003e-01],\n         [-1.6219e-01],\n         [-1.0723e-02],\n         [-1.5421e-01],\n         [-4.9339e-02],\n         [-3.8281e-02],\n         [-1.8616e-01],\n         [ 2.1373e-01],\n         [-8.6579e-02],\n         [-9.1772e-02],\n         [-1.0777e-01],\n         [-1.2135e-01],\n         [-8.8904e-02],\n         [-1.5278e-01],\n         [-1.2438e-01],\n         [-1.0036e-01],\n         [-1.1380e-01],\n         [-9.2139e-02],\n         [-8.9316e-02],\n         [-1.1768e-01]],\n\n        [[ 6.5625e-02],\n         [ 6.0019e-02],\n         [ 5.6963e-02],\n         [ 4.7871e-02],\n         [ 1.7319e-02],\n         [ 4.6699e-02],\n         [-1.6525e-01],\n         [ 6.3119e-02],\n         [-1.7403e-02],\n         [ 8.5018e-02],\n         [ 1.1315e-01],\n         [ 1.1513e-01],\n         [ 1.0047e-01],\n         [ 9.8939e-02],\n         [ 7.9134e-02],\n         [ 1.0424e-01],\n         [ 9.2704e-02],\n         [ 1.2133e-01],\n         [ 9.9248e-02],\n         [ 4.4340e-02],\n         [ 8.7588e-02],\n         [ 1.3086e-01],\n         [ 1.1863e-01],\n         [ 9.6424e-02]],\n\n        [[ 5.2794e-02],\n         [ 1.0923e-01],\n         [ 1.1409e-01],\n         [ 7.6880e-02],\n         [ 5.6569e-02],\n         [ 6.6162e-02],\n         [ 1.1882e-01],\n         [-7.2202e-02],\n         [ 9.9407e-02],\n         [ 1.9248e-01],\n         [ 1.1154e-01],\n         [ 1.1839e-01],\n         [ 1.3504e-01],\n         [ 1.4178e-01],\n         [ 1.5760e-01],\n         [ 1.1420e-01],\n         [ 1.0585e-01],\n         [ 1.4212e-01],\n         [ 1.2363e-01],\n         [ 7.6701e-02],\n         [ 1.2803e-01],\n         [ 1.4405e-01],\n         [ 1.5990e-01],\n         [ 1.7584e-01]],\n\n        [[-1.7807e-01],\n         [-2.6471e-01],\n         [-2.6448e-01],\n         [-2.6246e-01],\n         [-1.7450e-01],\n         [-1.4318e-01],\n         [-2.0536e-01],\n         [-2.7054e-01],\n         [-2.6496e-01],\n         [ 2.3996e-02],\n         [-1.9643e-01],\n         [-1.8517e-01],\n         [-1.5990e-01],\n         [-1.4904e-01],\n         [-1.7138e-01],\n         [-1.4573e-01],\n         [-1.5216e-01],\n         [-1.3528e-01],\n         [-1.4958e-01],\n         [-1.6462e-01],\n         [-1.6598e-01],\n         [-2.1219e-01],\n         [-2.1703e-01],\n         [-1.9016e-01]],\n\n        [[-8.1026e-02],\n         [ 1.5849e-02],\n         [ 4.3204e-02],\n         [ 4.3463e-02],\n         [-9.3584e-03],\n         [ 5.5187e-03],\n         [ 3.1684e-02],\n         [ 3.5463e-02],\n         [ 3.6369e-02],\n         [ 1.6242e-02],\n         [ 2.1057e-01],\n         [ 6.2618e-02],\n         [ 4.8180e-02],\n         [ 1.1223e-01],\n         [ 8.8505e-02],\n         [ 9.5567e-02],\n         [ 6.4552e-02],\n         [ 3.1516e-02],\n         [ 1.1724e-01],\n         [ 7.3821e-02],\n         [ 4.7713e-02],\n         [ 6.2399e-02],\n         [ 7.8784e-02],\n         [ 7.6747e-02]],\n\n        [[-1.1578e-01],\n         [-1.4448e-01],\n         [-1.2108e-01],\n         [-1.1716e-01],\n         [-1.3413e-01],\n         [-6.0049e-02],\n         [-1.0755e-01],\n         [-1.5272e-01],\n         [-1.4546e-01],\n         [-2.6533e-01],\n         [-1.5269e-01],\n         [ 1.7279e-01],\n         [-1.9391e-01],\n         [-8.6409e-02],\n         [-1.7555e-01],\n         [-1.7146e-01],\n         [-1.5573e-01],\n         [-1.4953e-01],\n         [-2.7697e-02],\n         [-1.7865e-01],\n         [-1.1629e-01],\n         [-6.9994e-02],\n         [-9.2495e-02],\n         [-1.9928e-01]],\n\n        [[ 3.9459e-02],\n         [-5.4832e-02],\n         [-6.4811e-02],\n         [-8.4612e-02],\n         [-2.3616e-02],\n         [-5.4199e-02],\n         [-7.1516e-02],\n         [-4.1795e-02],\n         [-6.5328e-02],\n         [ 2.9066e-01],\n         [ 1.4585e-01],\n         [ 1.6468e-01],\n         [ 1.3082e-01],\n         [ 1.3881e-01],\n         [ 1.1905e-01],\n         [ 1.1631e-01],\n         [ 1.1235e-01],\n         [ 9.4683e-02],\n         [ 1.6876e-01],\n         [ 1.5931e-01],\n         [ 1.6417e-01],\n         [ 1.6234e-01],\n         [ 1.1294e-01],\n         [ 1.8581e-01]],\n\n        [[-6.7158e-02],\n         [-6.1223e-02],\n         [-8.2407e-02],\n         [-6.6869e-02],\n         [-5.5211e-02],\n         [ 1.2791e-02],\n         [-6.7059e-02],\n         [ 1.9043e-01],\n         [ 1.2470e-01],\n         [ 1.0622e-01],\n         [ 1.1262e-01],\n         [ 1.1784e-01],\n         [ 1.2425e-01],\n         [ 1.4761e-01],\n         [ 1.1366e-01],\n         [ 1.2769e-01],\n         [ 1.8647e-01],\n         [ 1.6956e-01],\n         [ 1.0577e-01],\n         [ 1.8000e-01],\n         [ 1.7869e-01],\n         [ 1.5395e-01],\n         [ 1.2786e-01],\n         [ 1.0609e-01]],\n\n        [[-8.6342e-02],\n         [-1.2166e-01],\n         [-2.7074e-01],\n         [-1.4543e-01],\n         [-1.5476e-01],\n         [-1.1159e-01],\n         [-8.4526e-02],\n         [-2.3460e-02],\n         [-5.5933e-02],\n         [-3.3472e-02],\n         [-1.7022e-03],\n         [-6.1379e-02],\n         [-4.8252e-02],\n         [-3.3536e-02],\n         [-4.7247e-02],\n         [ 6.0844e-02],\n         [ 3.9172e-02],\n         [-1.0941e-01],\n         [ 3.3746e-02],\n         [ 4.2120e-02],\n         [ 7.4103e-02],\n         [ 6.7480e-02],\n         [-3.9778e-02],\n         [ 4.6653e-02]],\n\n        [[ 1.1761e-04],\n         [-3.2319e-02],\n         [-8.2931e-02],\n         [-6.2712e-02],\n         [-5.5349e-02],\n         [-1.6789e-01],\n         [-5.7528e-02],\n         [-3.3631e-02],\n         [-8.9793e-02],\n         [-4.4528e-02],\n         [ 1.9845e-01],\n         [ 4.1326e-02],\n         [ 6.2142e-02],\n         [ 5.2120e-02],\n         [ 4.5284e-02],\n         [ 5.3875e-03],\n         [ 2.3621e-02],\n         [ 4.3163e-02],\n         [ 2.7918e-02],\n         [-2.7312e-03],\n         [ 2.5573e-02],\n         [ 2.2228e-02],\n         [ 2.3738e-02],\n         [ 4.5626e-02]],\n\n        [[ 1.1152e-01],\n         [ 1.1857e-01],\n         [ 1.2849e-01],\n         [ 1.0917e-01],\n         [ 9.5948e-02],\n         [ 5.3384e-02],\n         [ 1.0204e-03],\n         [ 1.0989e-01],\n         [ 6.0001e-02],\n         [ 1.5550e-01],\n         [ 1.5263e-01],\n         [ 1.9080e-01],\n         [ 1.5649e-01],\n         [ 1.7876e-01],\n         [ 1.4794e-01],\n         [ 1.8362e-01],\n         [ 3.1785e-01],\n         [ 1.4816e-01],\n         [ 1.9398e-01],\n         [ 2.9620e-01],\n         [ 1.6768e-01],\n         [ 2.6444e-01],\n         [ 1.7876e-01],\n         [ 1.6529e-01]],\n\n        [[-3.2135e-01],\n         [-2.7609e-01],\n         [-2.2708e-01],\n         [-2.7871e-01],\n         [-1.5976e-01],\n         [-1.0789e-01],\n         [-2.6115e-01],\n         [ 2.4042e-01],\n         [ 1.7649e-01],\n         [ 1.0470e-01],\n         [ 7.6834e-02],\n         [ 1.1821e-01],\n         [ 9.2225e-02],\n         [ 1.8485e-01],\n         [ 1.6472e-01],\n         [ 1.1662e-01],\n         [ 1.3644e-01],\n         [ 1.0456e-01],\n         [ 1.4690e-01],\n         [ 1.0390e-01],\n         [ 1.6453e-01],\n         [ 1.6165e-01],\n         [ 1.5546e-01],\n         [ 1.7893e-01]],\n\n        [[ 2.2485e-01],\n         [ 1.5733e-01],\n         [ 1.8498e-01],\n         [ 1.1078e-01],\n         [ 2.0727e-01],\n         [ 1.8171e-01],\n         [ 4.9812e-02],\n         [ 1.6734e-02],\n         [ 1.8851e-01],\n         [ 2.8564e-01],\n         [ 2.0382e-01],\n         [ 1.7996e-01],\n         [ 1.8341e-01],\n         [ 1.6929e-01],\n         [ 1.5521e-01],\n         [ 1.3484e-01],\n         [ 1.5028e-01],\n         [ 1.4007e-01],\n         [ 9.8987e-02],\n         [ 1.9394e-01],\n         [ 2.3100e-01],\n         [ 2.1777e-01],\n         [ 1.4207e-01],\n         [ 3.0390e-01]],\n\n        [[-2.8275e-01],\n         [-1.7993e-01],\n         [-1.3902e-01],\n         [-1.6490e-01],\n         [-1.0496e-01],\n         [-2.0422e-01],\n         [-1.8531e-01],\n         [-1.7398e-01],\n         [-2.2590e-01],\n         [-1.9143e-01],\n         [ 3.7845e-01],\n         [-1.4968e-01],\n         [-1.7335e-01],\n         [-2.0042e-01],\n         [-1.4693e-01],\n         [-1.1373e-01],\n         [-4.5550e-02],\n         [-1.5862e-01],\n         [-1.9468e-01],\n         [-9.1910e-02],\n         [-2.0067e-01],\n         [-1.8242e-01],\n         [-2.3379e-01],\n         [-2.4639e-01]],\n\n        [[ 3.5746e-02],\n         [ 1.1198e-01],\n         [ 1.5230e-01],\n         [ 2.2968e-01],\n         [ 1.2334e-01],\n         [ 7.9760e-02],\n         [ 2.4281e-02],\n         [-1.0909e-02],\n         [ 1.1329e-01],\n         [ 2.1473e-01],\n         [ 1.6258e-01],\n         [ 1.7210e-01],\n         [ 1.3568e-01],\n         [ 9.7467e-02],\n         [ 8.7748e-02],\n         [ 1.7078e-01],\n         [ 1.8598e-01],\n         [ 8.9025e-02],\n         [ 1.1402e-01],\n         [ 1.6093e-01],\n         [ 2.7516e-01],\n         [ 1.6528e-01],\n         [ 1.5849e-01],\n         [ 1.4951e-01]],\n\n        [[-6.7726e-02],\n         [-2.6111e-02],\n         [-3.7656e-02],\n         [-3.9180e-02],\n         [-2.5971e-02],\n         [-1.4467e-01],\n         [-1.0697e-02],\n         [-1.7789e-02],\n         [-8.8801e-02],\n         [-1.5108e-01],\n         [-3.3329e-02],\n         [ 2.3654e-01],\n         [ 2.0878e-02],\n         [ 2.2508e-02],\n         [ 2.8622e-02],\n         [-1.1708e-02],\n         [-4.0135e-02],\n         [ 2.9418e-02],\n         [-9.4731e-03],\n         [-2.9228e-03],\n         [-2.7782e-02],\n         [ 2.2871e-02],\n         [ 1.7818e-02],\n         [ 2.9181e-02]],\n\n        [[ 4.7783e-02],\n         [ 8.7086e-02],\n         [ 7.9555e-02],\n         [ 6.4638e-02],\n         [ 1.0188e-02],\n         [-7.4722e-03],\n         [ 7.2756e-02],\n         [ 7.3916e-02],\n         [ 7.0337e-02],\n         [ 1.6638e-01],\n         [ 9.5175e-02],\n         [ 8.1817e-02],\n         [ 1.5183e-01],\n         [ 7.3419e-02],\n         [ 1.0479e-01],\n         [ 8.2887e-02],\n         [ 6.3357e-02],\n         [ 1.0268e-01],\n         [ 1.0171e-01],\n         [ 7.6174e-02],\n         [ 8.9681e-02],\n         [ 9.0263e-02],\n         [ 7.2253e-02],\n         [ 4.0903e-02]],\n\n        [[-1.8114e-02],\n         [-3.1521e-02],\n         [-3.0011e-02],\n         [-3.3700e-02],\n         [-3.9190e-02],\n         [-7.1181e-02],\n         [-1.3108e-01],\n         [-2.4210e-02],\n         [-3.9979e-02],\n         [-5.0791e-02],\n         [-7.0238e-03],\n         [-4.6672e-02],\n         [ 3.8189e-01],\n         [-3.8349e-02],\n         [-8.6204e-02],\n         [-3.9688e-02],\n         [ 1.9305e-02],\n         [-6.7339e-02],\n         [-3.3100e-02],\n         [ 5.0904e-03],\n         [-1.5047e-02],\n         [-4.9961e-04],\n         [-1.9295e-02],\n         [-1.3981e-02]],\n\n        [[ 1.4099e-02],\n         [ 6.8466e-02],\n         [ 7.5729e-02],\n         [ 6.5672e-02],\n         [ 5.7519e-02],\n         [ 1.3579e-03],\n         [ 9.5770e-04],\n         [ 1.6213e-02],\n         [ 1.4458e-02],\n         [ 9.2938e-04],\n         [ 7.3183e-04],\n         [ 2.2929e-02],\n         [ 1.6425e-01],\n         [ 1.3149e-01],\n         [ 1.3060e-01],\n         [ 1.2997e-01],\n         [ 1.4727e-01],\n         [ 1.5043e-01],\n         [ 1.4297e-01],\n         [ 1.5185e-01],\n         [ 1.2384e-01],\n         [ 1.8071e-01],\n         [ 9.4006e-02],\n         [ 1.1730e-01]],\n\n        [[-2.9610e-01],\n         [-2.3449e-01],\n         [-2.2987e-01],\n         [-2.7641e-01],\n         [-2.4688e-01],\n         [-3.2661e-01],\n         [-2.6682e-01],\n         [-2.5042e-01],\n         [-3.3929e-01],\n         [-4.5027e-01],\n         [-2.1687e-01],\n         [-2.9021e-01],\n         [-2.4233e-01],\n         [ 2.8537e-01],\n         [-2.8036e-01],\n         [-2.9515e-01],\n         [-3.0914e-01],\n         [-3.1012e-01],\n         [-3.1859e-01],\n         [-3.0750e-01],\n         [-3.0444e-01],\n         [-2.5483e-01],\n         [-2.9749e-01],\n         [-2.7640e-01]],\n\n        [[-5.2741e-01],\n         [-4.1537e-01],\n         [-3.7248e-01],\n         [-3.3458e-01],\n         [-2.7509e-01],\n         [-3.5539e-01],\n         [-4.7727e-01],\n         [-7.2280e-01],\n         [-6.3663e-01],\n         [-6.8911e-01],\n         [-5.5427e-01],\n         [-3.6727e-01],\n         [ 1.5880e-01],\n         [-5.3511e-01],\n         [-5.0268e-01],\n         [-5.0363e-01],\n         [-3.7419e-01],\n         [-3.0913e-01],\n         [-5.0107e-01],\n         [-3.5062e-01],\n         [-5.1421e-01],\n         [-5.7398e-01],\n         [-7.1038e-01],\n         [-5.1138e-01]],\n\n        [[-4.6731e-01],\n         [-4.7546e-01],\n         [-4.7340e-01],\n         [-5.3868e-01],\n         [-4.6113e-01],\n         [-4.7867e-01],\n         [-5.9746e-01],\n         [-5.4122e-01],\n         [-5.6321e-01],\n         [-4.7033e-01],\n         [-4.6819e-01],\n         [ 7.6297e-02],\n         [-4.5826e-01],\n         [-4.3651e-01],\n         [-4.4497e-01],\n         [-4.8697e-01],\n         [-4.5469e-01],\n         [-4.2812e-01],\n         [-4.5190e-01],\n         [-4.4646e-01],\n         [-4.7458e-01],\n         [-4.2964e-01],\n         [-4.4461e-01],\n         [-4.3520e-01]],\n\n        [[ 2.3901e-02],\n         [-8.3929e-02],\n         [-5.9767e-02],\n         [-7.8982e-02],\n         [-9.1446e-02],\n         [-7.5392e-02],\n         [-8.0575e-02],\n         [-1.6147e-01],\n         [-1.3367e-01],\n         [-1.0356e-01],\n         [ 2.7246e-01],\n         [-6.2741e-02],\n         [-8.1526e-02],\n         [-8.0032e-02],\n         [-1.0672e-01],\n         [-1.1291e-01],\n         [-1.1773e-01],\n         [-1.0829e-01],\n         [-9.8497e-02],\n         [-8.1571e-02],\n         [-8.9528e-02],\n         [-5.8892e-02],\n         [-6.5702e-02],\n         [-6.0051e-02]],\n\n        [[ 4.3231e-01],\n         [ 3.1132e-01],\n         [ 3.0673e-01],\n         [ 3.0883e-01],\n         [ 3.2686e-01],\n         [ 4.0427e-01],\n         [ 2.9928e-01],\n         [ 2.4772e-01],\n         [ 2.8525e-01],\n         [ 2.3264e-01],\n         [ 3.2084e-01],\n         [ 4.6697e-01],\n         [ 3.0005e-01],\n         [ 3.2966e-01],\n         [ 3.1251e-01],\n         [ 2.9748e-01],\n         [ 2.9741e-01],\n         [ 3.1698e-01],\n         [ 3.3682e-01],\n         [ 3.0614e-01],\n         [ 2.9851e-01],\n         [ 3.3857e-01],\n         [ 3.3640e-01],\n         [ 3.2906e-01]],\n\n        [[ 3.7814e-01],\n         [ 2.7629e-01],\n         [ 2.7702e-01],\n         [ 2.7938e-01],\n         [ 3.0291e-01],\n         [ 4.2052e-01],\n         [ 3.9039e-01],\n         [ 2.7374e-01],\n         [ 2.7679e-01],\n         [ 2.8592e-01],\n         [ 3.5162e-01],\n         [ 3.4790e-01],\n         [ 3.0891e-01],\n         [ 3.0513e-01],\n         [ 3.5150e-01],\n         [ 3.5958e-01],\n         [ 4.0797e-01],\n         [ 3.0780e-01],\n         [ 3.3473e-01],\n         [ 4.5103e-01],\n         [ 3.3080e-01],\n         [ 3.7032e-01],\n         [ 3.1657e-01],\n         [ 3.1476e-01]],\n\n        [[ 2.6813e-01],\n         [ 2.0210e-01],\n         [ 2.1139e-01],\n         [ 1.7551e-01],\n         [ 1.8097e-01],\n         [ 1.8400e-01],\n         [ 1.7217e-01],\n         [ 1.8994e-01],\n         [ 4.3222e-01],\n         [ 2.5479e-01],\n         [ 2.0056e-01],\n         [ 2.4933e-01],\n         [ 2.1955e-01],\n         [ 2.4208e-01],\n         [ 1.8911e-01],\n         [ 1.9604e-01],\n         [ 1.7348e-01],\n         [ 2.9323e-01],\n         [ 2.9433e-01],\n         [ 2.2136e-01],\n         [ 3.0027e-01],\n         [ 3.0404e-01],\n         [ 3.4177e-01],\n         [ 3.1198e-01]],\n\n        [[ 3.2656e-01],\n         [ 2.8771e-01],\n         [ 2.9052e-01],\n         [ 2.7728e-01],\n         [ 4.5873e-01],\n         [ 2.4484e-01],\n         [ 2.7377e-01],\n         [ 4.9087e-01],\n         [ 3.1983e-01],\n         [ 2.7421e-01],\n         [ 2.1840e-01],\n         [ 3.6485e-01],\n         [ 5.9517e-01],\n         [ 4.3731e-01],\n         [ 5.3635e-01],\n         [ 4.7069e-01],\n         [ 4.1696e-01],\n         [ 6.4639e-01],\n         [ 4.2241e-01],\n         [ 3.5925e-01],\n         [ 3.2833e-01],\n         [ 3.7496e-01],\n         [ 3.5311e-01],\n         [ 3.7080e-01]],\n\n        [[ 8.1766e-02],\n         [-9.9239e-02],\n         [-1.4638e-01],\n         [-1.2569e-01],\n         [-3.8941e-02],\n         [-2.0013e-01],\n         [-1.3408e-01],\n         [-5.5633e-02],\n         [-7.6402e-02],\n         [-1.7483e-02],\n         [-7.9198e-02],\n         [-5.4625e-02],\n         [ 1.6237e-01],\n         [-1.0378e-01],\n         [-1.0815e-01],\n         [-9.4098e-02],\n         [-1.6403e-01],\n         [-1.1638e-01],\n         [-1.1846e-01],\n         [-1.6589e-01],\n         [-1.4727e-01],\n         [-1.1786e-01],\n         [-1.2803e-01],\n         [-9.5555e-02]],\n\n        [[ 6.6414e-01],\n         [ 6.5246e-01],\n         [ 6.4179e-01],\n         [ 5.2887e-01],\n         [ 6.3609e-01],\n         [ 6.3148e-01],\n         [ 5.5823e-01],\n         [ 6.4409e-01],\n         [ 8.3820e-02],\n         [ 5.5487e-01],\n         [ 5.5256e-01],\n         [ 5.9509e-01],\n         [ 5.1239e-01],\n         [ 5.3664e-01],\n         [ 5.0079e-01],\n         [ 5.6930e-01],\n         [ 5.5542e-01],\n         [ 5.2900e-01],\n         [ 5.4712e-01],\n         [ 5.9712e-01],\n         [ 5.5663e-01],\n         [ 6.3237e-01],\n         [ 6.4783e-01],\n         [ 5.5444e-01]]], device='cuda:0', grad_fn=<AddBackward0>), ([], [], []))\n:List trace inputs must have elements (toTypeInferredIValue at /opt/conda/conda-bld/pytorch_1579027003190/work/torch/csrc/jit/pybind_utils.h:293)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7fdd0ee39627 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x6e4c9f (0x7fdd40197c9f in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #2: <unknown function> + 0x769f4b (0x7fdd4021cf4b in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #3: torch::jit::tracer::trace(std::vector<c10::IValue, std::allocator<c10::IValue> >, std::function<std::vector<c10::IValue, std::allocator<c10::IValue> > (std::vector<c10::IValue, std::allocator<c10::IValue> >)> const&, std::function<std::string (at::Tensor const&)>, bool, torch::jit::script::Module*) + 0x4e6 (0x7fdd14a3ee26 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch.so)\nframe #4: <unknown function> + 0x7660e1 (0x7fdd402190e1 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #5: <unknown function> + 0x77ffb1 (0x7fdd40232fb1 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #6: <unknown function> + 0x28c076 (0x7fdd3fd3f076 in /home/aloui/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #7: _PyCFunction_FastCallDict + 0x154 (0x5633d295f304 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #8: <unknown function> + 0x199c5e (0x5633d29e6c5e in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #9: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #10: <unknown function> + 0x19335e (0x5633d29e035e in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #11: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #12: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #13: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #14: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #15: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #16: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #17: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #18: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #19: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #20: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #21: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #22: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #23: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #24: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #25: _PyEval_EvalFrameDefault + 0x10c9 (0x5633d2a0a5d9 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #26: PyEval_EvalCodeEx + 0x329 (0x5633d29e1a49 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #27: PyEval_EvalCode + 0x1c (0x5633d29e27ec in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #28: <unknown function> + 0x1ba227 (0x5633d2a07227 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #29: _PyCFunction_FastCallDict + 0x91 (0x5633d295f241 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #30: <unknown function> + 0x199b0c (0x5633d29e6b0c in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #31: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #32: _PyGen_Send + 0x256 (0x5633d29e9bc6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #33: _PyEval_EvalFrameDefault + 0x1445 (0x5633d2a0a955 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #34: _PyGen_Send + 0x256 (0x5633d29e9bc6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #35: _PyEval_EvalFrameDefault + 0x1445 (0x5633d2a0a955 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #36: _PyGen_Send + 0x256 (0x5633d29e9bc6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #37: _PyCFunction_FastCallDict + 0x115 (0x5633d295f2c5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #38: <unknown function> + 0x199b0c (0x5633d29e6b0c in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #39: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #40: <unknown function> + 0x193cfb (0x5633d29e0cfb in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #41: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #42: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #43: <unknown function> + 0x193cfb (0x5633d29e0cfb in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #44: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #45: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #46: <unknown function> + 0x192f26 (0x5633d29dff26 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #47: _PyFunction_FastCallDict + 0x3d8 (0x5633d29e1628 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #48: _PyObject_FastCallDict + 0x26f (0x5633d295f6cf in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #49: _PyObject_Call_Prepend + 0x63 (0x5633d2964143 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #50: PyObject_Call + 0x3e (0x5633d295f10e in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #51: _PyEval_EvalFrameDefault + 0x1aaf (0x5633d2a0afbf in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #52: <unknown function> + 0x1931f6 (0x5633d29e01f6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #53: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #54: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x10c9 (0x5633d2a0a5d9 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #56: <unknown function> + 0x19c744 (0x5633d29e9744 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #57: _PyCFunction_FastCallDict + 0x91 (0x5633d295f241 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #58: <unknown function> + 0x199b0c (0x5633d29e6b0c in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #59: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #60: <unknown function> + 0x1931f6 (0x5633d29e01f6 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #61: <unknown function> + 0x193f31 (0x5633d29e0f31 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #62: <unknown function> + 0x199be5 (0x5633d29e6be5 in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\nframe #63: _PyEval_EvalFrameDefault + 0x30a (0x5633d2a0981a in /home/aloui/miniconda3/envs/vilbert-mt/bin/python)\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(logdir='results/runs')\n",
    "input_ = (question, features, spatials, segment_ids, input_mask, image_mask, co_attention_mask, task_tokens)\n",
    "writer.add_graph(model, input_to_model=input_)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VILBertForVLTasks(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (task_embeddings): Embedding(20, 768)\n",
       "    )\n",
       "    (v_embeddings): BertImageEmbeddings(\n",
       "      (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "      (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (v_layer): ModuleList(\n",
       "        (0): BertImageLayer(\n",
       "          (attention): BertImageAttention(\n",
       "            (self): BertImageSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertImageSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertImageLayer(\n",
       "          (attention): BertImageAttention(\n",
       "            (self): BertImageSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertImageSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertImageLayer(\n",
       "          (attention): BertImageAttention(\n",
       "            (self): BertImageSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertImageSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertImageLayer(\n",
       "          (attention): BertImageAttention(\n",
       "            (self): BertImageSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertImageSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertImageLayer(\n",
       "          (attention): BertImageAttention(\n",
       "            (self): BertImageSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertImageSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertImageLayer(\n",
       "          (attention): BertImageAttention(\n",
       "            (self): BertImageSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertImageSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (c_layer): ModuleList(\n",
       "        (0): BertConnectionLayer(\n",
       "          (biattention): BertBiAttention(\n",
       "            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (biOutput): BertBiOutput(\n",
       "            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm1): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (LayerNorm2): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (q_dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (v_intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (v_output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (t_intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (t_output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertConnectionLayer(\n",
       "          (biattention): BertBiAttention(\n",
       "            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (biOutput): BertBiOutput(\n",
       "            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm1): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (LayerNorm2): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (q_dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (v_intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (v_output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (t_intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (t_output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertConnectionLayer(\n",
       "          (biattention): BertBiAttention(\n",
       "            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (biOutput): BertBiOutput(\n",
       "            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm1): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (LayerNorm2): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (q_dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (v_intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (v_output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (t_intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (t_output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertConnectionLayer(\n",
       "          (biattention): BertBiAttention(\n",
       "            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (biOutput): BertBiOutput(\n",
       "            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm1): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (LayerNorm2): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (q_dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (v_intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (v_output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (t_intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (t_output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertConnectionLayer(\n",
       "          (biattention): BertBiAttention(\n",
       "            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (biOutput): BertBiOutput(\n",
       "            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm1): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (LayerNorm2): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (q_dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (v_intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (v_output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (t_intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (t_output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertConnectionLayer(\n",
       "          (biattention): BertBiAttention(\n",
       "            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (biOutput): BertBiOutput(\n",
       "            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm1): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (LayerNorm2): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "            (q_dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (v_intermediate): BertImageIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (v_output): BertImageOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (t_intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (t_output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (t_pooler): BertTextPooler(\n",
       "      (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (v_pooler): BertImagePooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (cls): BertPreTrainingHeads(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=False)\n",
       "    )\n",
       "    (bi_seq_relationship): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (imagePredictions): BertImagePredictionHead(\n",
       "      (transform): BertImgPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=1601, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (vil_prediction): SimpleClassifier(\n",
       "    (logit_fc): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (1): GeLU()\n",
       "      (2): FusedLayerNorm(torch.Size([2048]), eps=1e-12, elementwise_affine=True)\n",
       "      (3): Linear(in_features=2048, out_features=3129, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (vil_prediction_gqa): SimpleClassifier(\n",
       "    (logit_fc): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "      (1): GeLU()\n",
       "      (2): FusedLayerNorm(torch.Size([2048]), eps=1e-12, elementwise_affine=True)\n",
       "      (3): Linear(in_features=2048, out_features=1533, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (vil_binary_prediction): SimpleClassifier(\n",
       "    (logit_fc): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (1): GeLU()\n",
       "      (2): FusedLayerNorm(torch.Size([2048]), eps=1e-12, elementwise_affine=True)\n",
       "      (3): Linear(in_features=2048, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (vil_logit): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (vil_tri_prediction): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  (vision_logit): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (linguisic_logit): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.9015, device='cuda:0', grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vil_prediction[0][2969]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2969, 2785, 2293,  878,   89, 2621, 2134, 2621, 1403,  425, 1027,  411,\n",
       "         425, 1403,  444, 1076, 2218, 2621,  402, 2621, 1136,  425,  711, 2785,\n",
       "        2681, 2681,  990,  425, 1684, 2594], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hots[0][2969]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 3129])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
